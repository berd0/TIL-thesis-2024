{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T09:32:39.848472Z",
     "start_time": "2024-07-19T09:32:35.138232Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836cadbb1a3a569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T09:32:39.851951Z",
     "start_time": "2024-07-19T09:32:39.849477Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc5908638f4b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:41.077610Z",
     "start_time": "2024-07-19T11:54:41.074549Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "RESOLUTION = 10\n",
    "EMBEDDING_DIM = 200     # embedding dim equal to urban2vec & m3g study\n",
    "NUM_EPOCHS = 400       # Increased number of epochs\n",
    "BATCH_SIZE = 64         # Increased batch size\n",
    "LEARNING_RATE = 5e-6    # nice and slow\n",
    "CHECKPOINT_DIR = f'checkpoints_urban2vec_res{RESOLUTION}_dim{EMBEDDING_DIM}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb363d4e9ed85a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:42.193352Z",
     "start_time": "2024-07-19T11:54:42.190500Z"
    }
   },
   "outputs": [],
   "source": [
    "# # prepare poi unbuffered features for export\n",
    "# from srai.loaders import OSMPbfLoader\n",
    "# from srai.loaders.osm_loaders.filters import GEOFABRIK_LAYERS\n",
    "# \n",
    "# tags = GEOFABRIK_LAYERS\n",
    "# loader = OSMPbfLoader()\n",
    "# regions_gdf = gpd.read_file(f\"selected_regions_{RESOLUTION}.geojson\").set_index(\"region_id\")\n",
    "# features_gdf = loader.load(regions_gdf, tags)\n",
    "# \n",
    "# # Convert geometry to WKT for Parquet storage\n",
    "# features_gdf['geometry'] = features_gdf['geometry'].apply(lambda geom: geom.wkt)\n",
    "# # Export to Parquet\n",
    "# features_gdf.to_parquet(f\"POI_features_unbuffered_{RESOLUTION}.parquet\", index=True)\n",
    "# print(f\"Exported POI features to POI_features_unbuffered_{RESOLUTION}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae33fb7d076a9a9",
   "metadata": {},
   "source": [
    "Word2vec-style approach:\n",
    "In this approach, we treat each POI in a region as an individual \"word\" in the \"sentence\" (region). The model learns to predict the context (other POIs in the region) given a target POI, or vice versa. Negative sampling based on frequency from the entire corpus of words (not just local hexagons). This approach is more similar hex2vec than 2step ring sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65b1e99fda1f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.062045Z",
     "start_time": "2024-07-19T11:54:43.057194Z"
    }
   },
   "outputs": [],
   "source": [
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m=0.25, gamma=256):\n",
    "        super().__init__()\n",
    "        self.m, self.gamma = m, gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        sp = torch.sum(anchor * positive, dim=1)\n",
    "        sn = torch.sum(anchor * negative, dim=1)\n",
    "\n",
    "        ap = torch.clamp_min(-sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "\n",
    "        delta_p, delta_n = 1 - self.m, self.m\n",
    "        logit_p = -ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "\n",
    "        return self.soft_plus(logit_n + logit_p).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43254fceade98cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.070298Z",
     "start_time": "2024-07-19T11:54:43.063054Z"
    }
   },
   "outputs": [],
   "source": [
    "class Urban2VecDataset(Dataset):\n",
    "    def __init__(self, regions_gdf, joint_gdf, features_df):\n",
    "        # Create region dictionary\n",
    "        self.regions = list(regions_gdf.index)\n",
    "        self.region_to_idx = {r: i for i, r in enumerate(self.regions)}\n",
    "\n",
    "        # Collect all POIs across all features\n",
    "        all_pois = set()\n",
    "        for poi_list in features_df['poi_list']:\n",
    "            all_pois.update(poi_list)  # Add unique POIs from each list\n",
    "\n",
    "        # Create POI dictionary\n",
    "        self.poi_dictionary = sorted(all_pois)\n",
    "        self.poi_to_idx = {poi: idx for idx, poi in enumerate(self.poi_dictionary)}\n",
    "\n",
    "        self.num_pois = len(self.poi_dictionary)  # Store num_pois as attribute\n",
    "\n",
    "        # Create region to POIs mapping, filtering POIs not in poi_dictionary\n",
    "        self.region_to_pois = defaultdict(list)\n",
    "        for (region_id, feature_id) in joint_gdf.index:\n",
    "            region_idx = self.region_to_idx[region_id]\n",
    "            poi_list = features_df.loc[feature_id, 'poi_list']\n",
    "            filtered_poi_list = [self.poi_to_idx[poi] for poi in poi_list if poi in self.poi_to_idx]\n",
    "            self.region_to_pois[region_idx].extend(filtered_poi_list)  # Add filtered list\n",
    "\n",
    "        # Create list of valid regions (regions with POIs)\n",
    "        self.valid_regions = [idx for idx, pois in self.region_to_pois.items() if pois]\n",
    "\n",
    "        # Printing for debugging\n",
    "        print(f\"Number of valid regions: {len(self.valid_regions)}\")\n",
    "        print(f\"Number of unique POIs: {len(self.poi_dictionary)}\")\n",
    "\n",
    "        # Calculate POI frequencies for negative sampling\n",
    "        poi_counts = np.zeros(len(self.poi_dictionary))\n",
    "        for pois in self.region_to_pois.values():\n",
    "            for poi in pois:\n",
    "                poi_counts[poi] += 1\n",
    "        self.neg_sample_dist = np.power(poi_counts, 0.75)\n",
    "        self.neg_sample_dist /= self.neg_sample_dist.sum()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_regions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        region_idx = self.valid_regions[idx]\n",
    "        pois = self.region_to_pois[region_idx]\n",
    "\n",
    "        # assert pois, f\"Region {region_idx} has no valid POIs.\"  # Add assertion\n",
    "\n",
    "        positive_poi = np.random.choice(pois)\n",
    "        negative_poi = np.random.choice(len(self.poi_dictionary), p=self.neg_sample_dist) % self.num_pois\n",
    "\n",
    "        # # Print statements for debugging\n",
    "        # print(f\"Region index: {region_idx}, Max region index: {self.get_num_regions() - 1}\")\n",
    "        # print(f\"Positive POI index: {positive_poi}, Max POI index: {self.get_num_pois() - 1}\")\n",
    "        # print(f\"Negative POI index: {negative_poi}, Max POI index: {self.get_num_pois() - 1}\")\n",
    "        # \n",
    "        # assert 0 <= positive_poi < self.num_pois, f\"Invalid positive POI index: {positive_poi}\"\n",
    "        # assert 0 <= negative_poi < self.num_pois, f\"Invalid negative POI index: {negative_poi}\"\n",
    "\n",
    "        return region_idx, positive_poi, negative_poi\n",
    "\n",
    "    def get_num_regions(self):\n",
    "        return len(self.regions)\n",
    "\n",
    "    def get_num_pois(self):\n",
    "        return len(self.poi_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a254d660482e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6f54cb49414ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.074448Z",
     "start_time": "2024-07-19T11:54:43.070804Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    region_idx, positive_poi, negative_poi = zip(*batch)\n",
    "    return (\n",
    "        torch.tensor(region_idx, dtype=torch.long),\n",
    "        torch.tensor(positive_poi, dtype=torch.long),\n",
    "        torch.tensor(negative_poi, dtype=torch.long)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a2daede42487d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.103480Z",
     "start_time": "2024-07-19T11:54:43.099453Z"
    }
   },
   "outputs": [],
   "source": [
    "class Urban2VecModel(nn.Module):\n",
    "    def __init__(self, num_regions, num_pois, embedding_dim):\n",
    "        super(Urban2VecModel, self).__init__()\n",
    "        self.region_embedding = nn.Embedding(num_regions, embedding_dim)\n",
    "        self.poi_embedding = nn.Embedding(num_pois, embedding_dim)\n",
    "\n",
    "    def forward(self, region_indices, poi_indices):\n",
    "        region_embed = self.region_embedding(region_indices)\n",
    "        poi_embed = self.poi_embedding(poi_indices)\n",
    "        return region_embed, poi_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0880124a34eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.469958Z",
     "start_time": "2024-07-19T11:54:43.464551Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    regions_gdf = gpd.read_file(f\"selected_regions_{RESOLUTION}.geojson\").set_index(\"region_id\")\n",
    "    features_df = pd.read_parquet(f\"POI_features_unbuffered_{RESOLUTION}.parquet\")\n",
    "    \n",
    "    # use features_df NOT gdf. Filter out columns with POI catagories and get all non None values.\n",
    "    poi_columns = [col for col in features_df.columns if col != 'geometry']\n",
    "    features_df['poi_list'] = features_df[poi_columns].apply(lambda row: [val for val in row if pd.notna(val) and val != 'None'], axis=1)\n",
    "    features_df = features_df[features_df['poi_list'].map(len) > 0]\n",
    "    \n",
    "    # Make features_gdf from features_df by adding geometry\n",
    "    features_gdf = gpd.GeoDataFrame(\n",
    "        features_df,\n",
    "        geometry=gpd.GeoSeries.from_wkt(features_df['geometry']),\n",
    "        crs=regions_gdf.crs\n",
    "    )\n",
    "\n",
    "    # use IntersectionJoiner to join regions_gdf and features_gdf for sampling later\n",
    "    joiner = IntersectionJoiner()\n",
    "    joint_gdf = joiner.transform(regions_gdf, features_gdf)\n",
    "\n",
    "    return regions_gdf, joint_gdf, features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f081ba6e9b99e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.490607Z",
     "start_time": "2024-07-19T11:54:43.485474Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_urban2vec(model, dataloader, optimizer, criterion, device, num_epochs):\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    avg_loss_queue = deque(maxlen=10)  # Queue to store last 10 epoch losses\n",
    "\n",
    "    with tqdm(total=len(dataloader) * num_epochs, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:  # No tqdm here\n",
    "                region_idx, positive_poi, negative_poi = batch\n",
    "                region_idx, positive_poi, negative_poi = region_idx.to(device), positive_poi.to(device), negative_poi.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                region_embed = model.region_embedding(region_idx)\n",
    "                positive_embed = model.poi_embedding(positive_poi)\n",
    "                negative_embed = model.poi_embedding(negative_poi)\n",
    "\n",
    "                loss = criterion(region_embed, positive_embed, negative_embed)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                loss_values.append(loss.item())\n",
    "\n",
    "                # Update progress bar after each batch\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Calculate and display average epoch loss\n",
    "            avg_epoch_loss = total_loss / len(dataloader)\n",
    "            avg_loss_queue.append(avg_epoch_loss)\n",
    "\n",
    "            # Calculate 10-epoch running average\n",
    "            running_avg_loss = sum(avg_loss_queue) / len(avg_loss_queue)\n",
    "            pbar.set_postfix(loss=running_avg_loss)  # Show running average in tqdm\n",
    "\n",
    "    return model, loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ad7396ad4fef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.525843Z",
     "start_time": "2024-07-19T11:54:43.523114Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_pca(embeddings, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb57449d681c080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.542573Z",
     "start_time": "2024-07-19T11:54:43.539346Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(regions, embedding_file, n_components=EMBEDDING_DIM):\n",
    "    pretrained_embeddings = pd.read_csv(embedding_file, index_col=0)\n",
    "    pretrained_embeddings = pretrained_embeddings.reindex(regions)  # Align with current regions\n",
    "    pretrained_embeddings = pretrained_embeddings.fillna(0)  # Fill missing values\n",
    "\n",
    "    # Check if there are enough embeddings for PCA\n",
    "    if pretrained_embeddings.shape[1] >= n_components:\n",
    "        reduced_embeddings = apply_pca(pretrained_embeddings.values, n_components=n_components)\n",
    "    else:\n",
    "        print(\"Not enough pre-trained embeddings for PCA. Using as is.\")\n",
    "        reduced_embeddings = pretrained_embeddings.values\n",
    "\n",
    "    return torch.tensor(reduced_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f97097d29f3c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:43.556789Z",
     "start_time": "2024-07-19T11:54:43.550575Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    regions_gdf, joint_gdf, features_df = load_data()\n",
    "\n",
    "    dataset = Urban2VecDataset(regions_gdf, joint_gdf, features_df)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "    num_regions = dataset.get_num_regions()\n",
    "    num_pois = dataset.get_num_pois()\n",
    "    model = Urban2VecModel(num_regions, num_pois, EMBEDDING_DIM).to(device)\n",
    "\n",
    "    # Load pre-trained embeddings with proper handling and debugging prints\n",
    "    embedding_file = f\"embeddings_aerial_{RESOLUTION}_finetune.csv\"\n",
    "    try:\n",
    "        print(f\"Loading pretrained embeddings from: {embedding_file}\")\n",
    "        pretrained_embeddings = load_pretrained_embeddings(dataset.regions, embedding_file)\n",
    "        print(f\"pretrained_embeddings shape: {pretrained_embeddings.shape}\")\n",
    "\n",
    "        model.region_embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        print(\"Loaded and PCA-reduced/used as is pre-trained region embeddings.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pre-trained embeddings not found. Initializing randomly.\")\n",
    "\n",
    "    # Print for debugging\n",
    "    print(f\"Region embeddings shape: {model.region_embedding.weight.shape}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = CircleLoss()\n",
    "\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    loss_values = []  # Initialize loss_values here\n",
    "    try:\n",
    "        model, loss_values = train_urban2vec(model, dataloader, optimizer, criterion, device, NUM_EPOCHS)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"RuntimeError during training: {e}\")\n",
    "        print(\"Consider setting CUDA_LAUNCH_BLOCKING=1 for more detailed error messages.\")\n",
    "\n",
    "    region_embeddings = model.region_embedding.weight.detach().cpu().numpy()\n",
    "    poi_embeddings = model.poi_embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "    pd.DataFrame(region_embeddings, index=dataset.regions).to_csv(os.path.join(CHECKPOINT_DIR, 'step2_region_embeddings.csv'))\n",
    "    pd.DataFrame(poi_embeddings, index=dataset.poi_dictionary).to_csv(os.path.join(CHECKPOINT_DIR, 'step2_poi_embeddings.csv'))\n",
    "\n",
    "    print(\"Training completed and embeddings saved.\")\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2baf6ef9d921769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:21:02.482949Z",
     "start_time": "2024-07-19T11:54:43.557793Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loss_values = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43b810c50cfd4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:21:02.791531Z",
     "start_time": "2024-07-19T12:21:02.483955Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot loss values and add a smoothened average line\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_values)\n",
    "plt.plot(pd.Series(loss_values).rolling(1000).mean())\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Circle Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282542845f159104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:21:11.434414Z",
     "start_time": "2024-07-19T12:21:02.793032Z"
    }
   },
   "outputs": [],
   "source": [
    "# import trained region embeddings\n",
    "region_embeddings = pd.read_csv(os.path.join(CHECKPOINT_DIR, 'step2_region_embeddings.csv'), index_col=0)\n",
    "regions_gdf = gpd.read_file(f\"selected_regions_{RESOLUTION}.geojson\").set_index(\"region_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43963870862e6103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:23:34.451254Z",
     "start_time": "2024-07-19T12:21:11.435416Z"
    }
   },
   "outputs": [],
   "source": [
    "from Plotting import pca_plot, cluster_agglomerative_plot, cluster_kmeans_plot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cluster_agglomerative_plot(region_embeddings, regions_gdf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7c80e3334a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441cf5504c717b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_test_gdf = pd.read_parquet(f\"POI_features_unbuffered_{RESOLUTION}.parquet\")\n",
    "# features_test_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19872eee254ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resume_training(checkpoint_path, model, optimizer, scheduler, start_epoch):\n",
    "#     if os.path.exists(checkpoint_path):\n",
    "#         checkpoint = torch.load(checkpoint_path)\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         if 'scheduler_state_dict' in checkpoint:\n",
    "#             scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#         else:\n",
    "#             print(\"Scheduler state not found in checkpoint. Using default scheduler.\")\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "#         print(f\"Resuming training from epoch {start_epoch}\")\n",
    "#     else:\n",
    "#         print(\"No checkpoint found. Starting training from scratch.\")\n",
    "#     return model, optimizer, scheduler, start_epoch\n",
    "# \n",
    "# # Usage:\n",
    "# checkpoint_path = os.path.join(CHECKPOINT_DIR, 'checkpoint_epoch_22.pth')  # or 'best_model.pth'\n",
    "# model, optimizer, scheduler, start_epoch = resume_training(checkpoint_path, model, optimizer, scheduler, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f146bc38a5edd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plotting embeddings for test\n",
    "# from Plotting import pca_plot, cluster_agglomerative_plot, cluster_kmeans_plot\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# embeddings_reduced_df = pca.fit_transform(embedding_df)\n",
    "# # print variance explained\n",
    "# print(f\"Variance explained: {pca.explained_variance_ratio_.sum()*100}%\")\n",
    "# # ensure the index is preserved\n",
    "# embeddings_reduced_df = pd.DataFrame(embeddings_reduced_df, index=embedding_df.index)\n",
    "# cluster_agglomerative_plot(embeddings_reduced_df, regions_gdf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68428c37cfaaad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:24:24.041189Z",
     "start_time": "2024-07-19T12:23:34.474466Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Ensure the regions_gdf and embeddings_aerial, embedding_df are loaded correctly\n",
    "# Assuming regions_gdf['afw', 'fys', 'onv', 'soc', 'vrz', 'won'] contains the scores\n",
    "\n",
    "# Get the Leefbaarometer scores\n",
    "scores_df = regions_gdf[['afw', 'fys', 'onv', 'soc', 'vrz', 'won']]\n",
    "\n",
    "# Initialize lists to store R-squared values\n",
    "r2_scores_step1 = []\n",
    "r2_scores_step2 = []\n",
    "score_names = scores_df.columns\n",
    "\n",
    "# Loop over each score\n",
    "for score_name in score_names:\n",
    "    scores = scores_df[score_name]\n",
    "\n",
    "    # Get the embeddings\n",
    "    embeddings_step1 = pd.read_csv('embeddings_aerial_10_finetune.csv', index_col=0)\n",
    "    #embeddings_step1 = embeddings_aerial.loc[regions_gdf.index]\n",
    "    embeddings_step2 = region_embeddings.loc[regions_gdf.index]\n",
    "    #embeddings_step2 = embedding_df.loc[regions_gdf.index]\n",
    "    \n",
    "    # ensure same dimensionality with PCA\n",
    "    pca = PCA(n_components=30)\n",
    "    embeddings_step1 = pca.fit_transform(embeddings_step1)\n",
    "    embeddings_step2 = pca.fit_transform(embeddings_step2)\n",
    "    \n",
    "    # Fit the model for step 1\n",
    "    model_step1 = LinearRegression().fit(embeddings_step1, scores)\n",
    "    r2_step1 = r2_score(scores, model_step1.predict(embeddings_step1))\n",
    "    r2_scores_step1.append(r2_step1)\n",
    "\n",
    "    # Fit the model for step 2\n",
    "    model_step2 = LinearRegression().fit(embeddings_step2, scores)\n",
    "    r2_step2 = r2_score(scores, model_step2.predict(embeddings_step2))\n",
    "    r2_scores_step2.append(r2_step2)\n",
    "\n",
    "# Plotting the R-squared values\n",
    "x = np.arange(len(score_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar1 = ax.bar(x - width/2, r2_scores_step1, width, label='Step 1')\n",
    "bar2 = ax.bar(x + width/2, r2_scores_step2, width, label='Step 2')\n",
    "\n",
    "# Adding labels and titles\n",
    "ax.set_xlabel('Leefbaarometer Scores')\n",
    "ax.set_ylabel('R-squared Value')\n",
    "ax.set_title('R-squared Values of Embeddings Predicting Leefbaarometer Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(score_names)\n",
    "ax.legend()\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec2078a19a29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get r squared value of aerial embeddings to leefbaarometer scores in region_gdf (regions_gdf['afw'])\n",
    "# # get the scores\n",
    "# scores = regions_gdf['afw']\n",
    "# # get the embeddings\n",
    "# \n",
    "# # fit the model\n",
    "# model = LinearRegression().fit(embeddings, scores)\n",
    "# # get the r squared value\n",
    "# r2 = r2_score(scores, model.predict(embeddings))\n",
    "# print(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
