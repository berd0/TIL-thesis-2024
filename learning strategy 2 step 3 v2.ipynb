{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.007948Z",
     "start_time": "2024-07-23T11:47:43.399759Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from networkx import Graph\n",
    "from shapely.geometry import Point\n",
    "from collections import deque\n",
    "from sklearn.decomposition import PCA\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "import random\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a19922469d576",
   "metadata": {},
   "source": [
    "# Learning strategy 2: Step 3 - Training Region Embeddings with Circle Loss and distance-based adjacency graph\n",
    "\n",
    "This script performs the third step of learning strategy 2, focusing on training region embeddings using a neural network (nn.Embedding) and Circle Loss. The process incorporates the following key steps:\n",
    "\n",
    "1. **Data Loading and Preprocessing**:\n",
    "    - Loads geospatial data for specified regions using GeoPandas.\n",
    "    - Utilizes H3 hexagonal indices for spatial context with the SRAI library.\n",
    "\n",
    "2. **Adjacency Graph Creation**:\n",
    "    - Creates an adjacency graph considering neighbors up to a specified distance.\n",
    "    - Employs parallel processing using a ThreadPoolExecutor for efficiency.\n",
    "    - Supports Euclidean distance and location-based accessibility for adjacency calculations.\n",
    "\n",
    "3. **Training Neural Network**:\n",
    "    - Initializes region embeddings using pre-trained embeddings from aerial images and POIs.\n",
    "    - Utilizes nn.Embedding for the neural network.\n",
    "    - Employs Circle Loss for training, focusing on minimizing the distances between anchor-positive pairs while maximizing the distances between anchor-negative pairs.\n",
    "    - Samples triplets using a weighted graph-based random walk strategy.\n",
    "\n",
    "4. **Embedding and Graph Export**:\n",
    "    - Saves the final trained embeddings and the adjacency graph as Parquet files in the specified checkpoint directory.\n",
    "\n",
    "This approach ensures scalable and efficient processing for urban region embedding tasks, leveraging both spatial context and parallel computation for enhanced performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad074b51a06a057b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.013711Z",
     "start_time": "2024-07-23T11:47:48.009453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "RESOLUTION = 10\n",
    "EMBEDDING_DIM = 200\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 5e-5\n",
    "ADJACENCY_DISTANCE = 5\n",
    "NUM_WORKERS = 16\n",
    "DISTANCE_MEASURE = \"Location_based_accessibility\"  # or \"Euclidean\"\n",
    "WALK_LENGTH = 10\n",
    "NUM_ANTS = 1\n",
    "CHECKPOINT_DIR = f'checkpoints_urban2vec_res{RESOLUTION}_dim{EMBEDDING_DIM}'\n",
    "PRETRAINED_EMBEDDING_FILE = os.path.join(CHECKPOINT_DIR, \"step2_region_embeddings.csv\")\n",
    "CUTOFF_TIME = 60 * 60  # 60 minutes in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823f04b0936cb9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.021011Z",
     "start_time": "2024-07-23T11:47:48.014717Z"
    }
   },
   "outputs": [],
   "source": [
    "# CircleLoss definition\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m=0.25, gamma=256):\n",
    "        super().__init__()\n",
    "        self.m, self.gamma = m, gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        sp = torch.sum(anchor * positive, dim=1)\n",
    "        sn = torch.sum(anchor * negative, dim=1)\n",
    "\n",
    "        ap = torch.clamp_min(-sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "\n",
    "        delta_p, delta_n = 1 - self.m, self.m\n",
    "        logit_p = -ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "\n",
    "        return self.soft_plus(logit_n + logit_p).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816cb1a8ee573f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.033878Z",
     "start_time": "2024-07-23T11:47:48.022513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "class Urban2VecDataset(Dataset):\n",
    "    def __init__(self, regions_gdf, adjacency_graph, num_ants=NUM_ANTS, walk_length=WALK_LENGTH):\n",
    "        self.regions = list(regions_gdf.index)\n",
    "        self.region_to_idx = {r: i for i, r in enumerate(self.regions)}\n",
    "        self.adjacency_graph = adjacency_graph\n",
    "        self.num_ants = num_ants\n",
    "        self.walk_length = walk_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_region = self.regions[idx]\n",
    "\n",
    "        positive_regions = set()\n",
    "        for _ in range(self.num_ants):\n",
    "            current_region = anchor_region\n",
    "            for _ in range(self.walk_length):\n",
    "                if current_region not in self.adjacency_graph:\n",
    "                    break\n",
    "                neighbors = list(self.adjacency_graph.neighbors(current_region))\n",
    "                if not neighbors:\n",
    "                    break\n",
    "                weights = np.array([self.adjacency_graph[current_region][n]['weight'] for n in neighbors])\n",
    "                weights = 1 / weights\n",
    "                weights /= weights.sum()\n",
    "                next_region = np.random.choice(neighbors, p=weights)\n",
    "                positive_regions.add(next_region)\n",
    "                current_region = next_region\n",
    "\n",
    "        if not positive_regions:\n",
    "            positive_region = anchor_region\n",
    "        else:\n",
    "            positive_region = random.choice(list(positive_regions))\n",
    "\n",
    "        negative_region = random.choice([r for r in self.regions if r not in positive_regions and r != anchor_region])\n",
    "\n",
    "        anchor_idx = self.region_to_idx[anchor_region]\n",
    "        positive_idx = self.region_to_idx[positive_region]\n",
    "        negative_idx = self.region_to_idx[negative_region]\n",
    "\n",
    "        return anchor_idx, positive_idx, negative_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10068ba2225f9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.037703Z",
     "start_time": "2024-07-23T11:47:48.034879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom collate function\n",
    "def custom_collate(batch):\n",
    "    anchor_idx, positive_idx, negative_idx = zip(*batch)\n",
    "    return (\n",
    "        torch.tensor(anchor_idx, dtype=torch.long),\n",
    "        torch.tensor(positive_idx, dtype=torch.long),\n",
    "        torch.tensor(negative_idx, dtype=torch.long)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d25530866bb3a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.046312Z",
     "start_time": "2024-07-23T11:47:48.038705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Urban2VecModel definition\n",
    "class Urban2VecModel(nn.Module):\n",
    "    def __init__(self, num_regions, embedding_dim):\n",
    "        super(Urban2VecModel, self).__init__()\n",
    "        self.region_embedding = nn.Embedding(num_regions, embedding_dim)\n",
    "\n",
    "    def forward(self, region_indices):\n",
    "        region_embed = self.region_embedding(region_indices)\n",
    "        return region_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d736cdca0caf51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.077083Z",
     "start_time": "2024-07-23T11:47:48.047816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    regions_gdf = gpd.read_file(f\"selected_regions_{RESOLUTION}.geojson\").set_index(\"region_id\")\n",
    "    return regions_gdf\n",
    "\n",
    "def load_and_prepare_driving_graph():\n",
    "    print(\"Loading and preparing driving graph...\")\n",
    "    G_drive = ox.graph_from_place('south holland, Netherlands', network_type='drive')\n",
    "    G_drive = ox.project_graph(G_drive)\n",
    "\n",
    "    for u, v, k, data in G_drive.edges(data=True, keys=True):\n",
    "        speed_mps = data.get('speed_kph', 50) / 3.6  # Default to 50 km/h if speed not available\n",
    "        data['travel_time'] = data['length'] / speed_mps\n",
    "\n",
    "    return G_drive\n",
    "\n",
    "def create_nodes_kdtree(nodes_gdf):\n",
    "    nodes_xy = np.array(list(zip(nodes_gdf['geometry'].x, nodes_gdf['geometry'].y)))\n",
    "    return KDTree(nodes_xy)\n",
    "\n",
    "def map_centroids_to_network_nodes(regions_gdf, G):\n",
    "    print(\"Mapping centroids to network nodes...\")\n",
    "    nodes, _ = ox.graph_to_gdfs(G)\n",
    "    nodes_tree = KDTree(nodes[['x', 'y']])\n",
    "\n",
    "    centroid_node_mapping = {}\n",
    "    unmapped_regions = []\n",
    "\n",
    "    for idx, row in regions_gdf.iterrows():\n",
    "        x, y = row.geometry.centroid.x, row.geometry.centroid.y\n",
    "        distances, indices = nodes_tree.query([[x, y]], k=4)  # Get 4 nearest neighbors\n",
    "\n",
    "        mapped = False\n",
    "        for distance, index in zip(distances[0], indices[0]):\n",
    "            nearest_node = nodes.iloc[index].name\n",
    "            if nearest_node in G.nodes:\n",
    "                centroid_node_mapping[idx] = nearest_node\n",
    "                mapped = True\n",
    "                break\n",
    "\n",
    "        if not mapped:\n",
    "            unmapped_regions.append(idx)\n",
    "            print(f\"Warning: Could not map region {idx} to any nearby node in the graph.\")\n",
    "\n",
    "    print(f\"Successfully mapped {len(centroid_node_mapping)} out of {len(regions_gdf)} regions.\")\n",
    "    print(f\"Number of unmapped regions: {len(unmapped_regions)}\")\n",
    "\n",
    "    return centroid_node_mapping, unmapped_regions\n",
    "\n",
    "def calculate_region_density(regions_gdf):\n",
    "    print(\"Calculating region densities...\")\n",
    "\n",
    "    # Load building density data\n",
    "    building_density_gdf = gpd.read_file(r\"D:\\tu delft\\Afstuderen\\Phase 4 online triplet loss\\Rudifun_PV28_Zuid_Holland\\Rudifun_Bruto_Buurt_PV28.shp\")\n",
    "    building_density_gdf = building_density_gdf.to_crs(epsg=4326)  # Ensure it's in the same CRS as regions_gdf\n",
    "\n",
    "    # Project both GeoDataFrames to EPSG:28992 for accurate area calculation in square meters\n",
    "    regions_gdf_projected = regions_gdf.to_crs(epsg=28992)\n",
    "    building_density_gdf_projected = building_density_gdf.to_crs(epsg=28992)\n",
    "\n",
    "    # Perform spatial intersection on projected GeoDataFrames\n",
    "    intersections = gpd.overlay(regions_gdf_projected.reset_index(), building_density_gdf_projected, how='intersection')\n",
    "\n",
    "    # Calculate intersection area in square meters\n",
    "    intersections['IntersectionArea'] = intersections.geometry.area\n",
    "\n",
    "    # Calculate weighted building density based on intersection area\n",
    "    intersections['WeightedDensity'] = intersections['FSI_22'] * intersections['IntersectionArea']\n",
    "\n",
    "    # Aggregate weighted density and total intersection area by region\n",
    "    grouped = intersections.groupby('region_id').agg({'WeightedDensity': 'sum', 'IntersectionArea': 'sum'})\n",
    "\n",
    "    # Calculate the weighted average density\n",
    "    grouped['average_density'] = grouped['WeightedDensity'] / grouped['IntersectionArea']\n",
    "    grouped.fillna(0, inplace=True)\n",
    "\n",
    "    return grouped['average_density']\n",
    "\n",
    "def calculate_euclidean_distance(origin, destination, regions_gdf):\n",
    "    return Point(regions_gdf.loc[origin].geometry.centroid).distance(Point(regions_gdf.loc[destination].geometry.centroid))\n",
    "\n",
    "def calculate_location_based_accessibility(origin, destination, travel_time, density, decay_parameter=0.001):\n",
    "    return np.exp(-decay_parameter * travel_time) * density[destination]\n",
    "\n",
    "def calculate_adjacency_for_region(region, regions_gdf, h3_neighbourhood, distance, distance_measure, G_drive=None, centroid_node_mapping=None, density=None):\n",
    "    edges = []\n",
    "    neighbors = h3_neighbourhood.get_neighbours_up_to_distance(region, distance)\n",
    "\n",
    "    if distance_measure == \"Euclidean\":\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in regions_gdf.index:\n",
    "                dist = calculate_euclidean_distance(region, neighbor, regions_gdf)\n",
    "                edges.append((region, neighbor, dist))\n",
    "    elif distance_measure == \"Location_based_accessibility\":\n",
    "        if region not in centroid_node_mapping or centroid_node_mapping[region][0] not in G_drive:\n",
    "            print(f\"Warning: Node for region {region} not found in graph. Skipping.\")\n",
    "            return edges\n",
    "\n",
    "        origin_node = centroid_node_mapping[region][0]\n",
    "        try:\n",
    "            travel_times = nx.single_source_dijkstra_path_length(G_drive, origin_node, weight='travel_time', cutoff=CUTOFF_TIME)\n",
    "        except nx.NodeNotFound:\n",
    "            print(f\"Warning: Node {origin_node} for region {region} not found in graph. Skipping.\")\n",
    "            return edges\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in regions_gdf.index and neighbor in centroid_node_mapping:\n",
    "                destination_node = centroid_node_mapping[neighbor][0]\n",
    "                if destination_node in travel_times:\n",
    "                    travel_time = travel_times[destination_node]\n",
    "                    accessibility = calculate_location_based_accessibility(region, neighbor, travel_time, density)\n",
    "                    edges.append((region, neighbor, accessibility))\n",
    "                else:\n",
    "                    print(f\"Warning: No path found from {region} to {neighbor}. Skipping.\")\n",
    "\n",
    "    return edges\n",
    "\n",
    "def calculate_accessibility_edges(regions_gdf, h3_neighbourhood, distance, G_drive, centroid_node_mapping):\n",
    "    edges = []\n",
    "    for region in tqdm(regions_gdf.index, desc=\"Calculating accessibility\"):\n",
    "        origin_node = centroid_node_mapping[region]\n",
    "        neighbors = h3_neighbourhood.get_neighbours_up_to_distance(region, distance)\n",
    "        neighbors = [n for n in neighbors if n in regions_gdf.index]\n",
    "\n",
    "        travel_times = nx.single_source_dijkstra_path_length(G_drive, origin_node, weight='travel_time', cutoff=CUTOFF_TIME)\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            dest_node = centroid_node_mapping[neighbor]\n",
    "            if dest_node in travel_times:\n",
    "                travel_time = travel_times[dest_node]\n",
    "                accessibility = 1 / (1 + travel_time)  # Simple accessibility measure\n",
    "                edges.append((region, neighbor, accessibility))\n",
    "\n",
    "    return edges\n",
    "\n",
    "def calculate_euclidean_edges(regions_gdf, h3_neighbourhood, distance):\n",
    "    edges = []\n",
    "    for region in tqdm(regions_gdf.index, desc=\"Calculating Euclidean distances\"):\n",
    "        neighbors = h3_neighbourhood.get_neighbours_up_to_distance(region, distance)\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in regions_gdf.index:\n",
    "                dist = regions_gdf.loc[region].geometry.centroid.distance(regions_gdf.loc[neighbor].geometry.centroid)\n",
    "                edges.append((region, neighbor, dist))\n",
    "    return edges\n",
    "\n",
    "def create_adjacency_graph(regions_gdf, distance=ADJACENCY_DISTANCE, distance_measure=DISTANCE_MEASURE, G_drive=None):\n",
    "    print(\"Creating adjacency graph...\")\n",
    "    centroid_node_mapping, unmapped_regions = map_centroids_to_network_nodes(regions_gdf, G_drive)\n",
    "\n",
    "    # Calculate region densities\n",
    "    densities = calculate_region_density(regions_gdf)\n",
    "\n",
    "    # Check if all regions have density data\n",
    "    missing_density = set(regions_gdf.index) - set(densities.index)\n",
    "    if missing_density:\n",
    "        print(f\"Warning: {len(missing_density)} regions are missing density data\")\n",
    "\n",
    "    adjacency_matrix = pd.DataFrame(index=regions_gdf.index, columns=regions_gdf.index)\n",
    "\n",
    "    h3_neighbourhood = H3Neighbourhood(regions_gdf=regions_gdf, include_center=False)\n",
    "\n",
    "    for region in tqdm(regions_gdf.index, desc=\"Calculating accessibility\"):\n",
    "        if region in unmapped_regions or region not in densities.index:\n",
    "            continue\n",
    "\n",
    "        origin_node = centroid_node_mapping[region]\n",
    "        neighbors = h3_neighbourhood.get_neighbours_up_to_distance(region, distance)\n",
    "        neighbors = [n for n in neighbors if n in regions_gdf.index and n not in unmapped_regions and n in densities.index]\n",
    "\n",
    "        try:\n",
    "            travel_times = nx.single_source_dijkstra_path_length(G_drive, origin_node, weight='travel_time', cutoff=CUTOFF_TIME)\n",
    "        except nx.NodeNotFound:\n",
    "            print(f\"Error: Node {origin_node} not found in graph for region {region}.\")\n",
    "            continue\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            dest_node = centroid_node_mapping[neighbor]\n",
    "            if dest_node in travel_times:\n",
    "                travel_time = travel_times[dest_node]\n",
    "                accessibility = calculate_location_based_accessibility(region, neighbor, travel_time, densities)\n",
    "                adjacency_matrix.at[region, neighbor] = accessibility\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "def calculate_location_based_accessibility(origin, destination, travel_time, densities, decay_parameter=0.001):\n",
    "    try:\n",
    "        destination_density = densities.loc[destination]\n",
    "        return np.exp(-decay_parameter * travel_time) * destination_density\n",
    "    except KeyError:\n",
    "        print(f\"Warning: Density data not found for region {destination}\")\n",
    "        return 0  # or some other default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45244ad0793f34c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.082391Z",
     "start_time": "2024-07-23T11:47:48.078085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export adjacency graph\n",
    "def export_adjacency_graph(graph, filename):\n",
    "    print(\"Exporting adjacency graph...\")\n",
    "    adjacency_list = []\n",
    "    for node1, node2, data in graph.edges(data=True):\n",
    "        adjacency_list.append({'node1': node1, 'node2': node2, 'weight': data['weight']})\n",
    "    adjacency_df = pd.DataFrame(adjacency_list)\n",
    "    adjacency_df.to_parquet(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84b2af2ac67f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.089378Z",
     "start_time": "2024-07-23T11:47:48.082892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Urban2Vec model\n",
    "def train_urban2vec(model, dataloader, optimizer, criterion, device, num_epochs):\n",
    "    print(\"Starting training...\")\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    avg_loss_queue = deque(maxlen=10)\n",
    "\n",
    "    with tqdm(total=len(dataloader) * num_epochs, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                anchor_idx, positive_idx, negative_idx = batch\n",
    "                anchor_idx, positive_idx, negative_idx = anchor_idx.to(device), positive_idx.to(device), negative_idx.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                anchor_embed = model(anchor_idx)\n",
    "                positive_embed = model(positive_idx)\n",
    "                negative_embed = model(negative_idx)\n",
    "\n",
    "                loss = criterion(anchor_embed, positive_embed, negative_embed)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                loss_values.append(loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            avg_epoch_loss = total_loss / len(dataloader)\n",
    "            avg_loss_queue.append(avg_epoch_loss)\n",
    "\n",
    "            running_avg_loss = sum(avg_loss_queue) / len(avg_loss_queue)\n",
    "            pbar.set_postfix(loss=running_avg_loss)\n",
    "\n",
    "    return model, loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869510638a7b430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.094186Z",
     "start_time": "2024-07-23T11:47:48.091380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "def apply_pca(embeddings, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccf2bec71d5c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.099257Z",
     "start_time": "2024-07-23T11:47:48.094687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "def load_pretrained_embeddings(regions, embedding_file, n_components=EMBEDDING_DIM):\n",
    "    print(f\"Loading pretrained embeddings from: {embedding_file}\")\n",
    "    pretrained_embeddings = pd.read_csv(embedding_file, index_col=0)\n",
    "    pretrained_embeddings = pretrained_embeddings.reindex(regions)\n",
    "    pretrained_embeddings = pretrained_embeddings.fillna(0)\n",
    "\n",
    "    if pretrained_embeddings.shape[1] >= n_components:\n",
    "        reduced_embeddings = apply_pca(pretrained_embeddings.values, n_components=n_components)\n",
    "    else:\n",
    "        print(\"Not enough pre-trained embeddings for PCA. Using as is.\")\n",
    "        reduced_embeddings = pretrained_embeddings.values\n",
    "\n",
    "    return torch.tensor(reduced_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ddbfddfc693f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:47:48.103321Z",
     "start_time": "2024-07-23T11:47:48.100259Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load data and prepare graph\n",
    "# regions_gdf = load_data()\n",
    "# G_drive = load_and_prepare_driving_graph()\n",
    "# \n",
    "# # Print some information about the graph\n",
    "# print(f\"Number of nodes in G_drive: {len(G_drive.nodes)}\")\n",
    "# print(f\"Number of edges in G_drive: {len(G_drive.edges)}\")\n",
    "# \n",
    "# # Create adjacency matrix\n",
    "# adjacency_matrix = create_adjacency_graph(regions_gdf, distance=ADJACENCY_DISTANCE, distance_measure=DISTANCE_MEASURE, G_drive=G_drive)\n",
    "# \n",
    "# # Print some information about the adjacency matrix\n",
    "# print(f\"Shape of adjacency matrix: {adjacency_matrix.shape}\")\n",
    "# print(f\"Number of non-null values: {adjacency_matrix.notna().sum().sum()}\")\n",
    "# \n",
    "# # save as parquet\n",
    "# adjacency_matrix.to_parquet(os.path.join(CHECKPOINT_DIR, f'adjacency_matrix_res{RESOLUTION}_{DISTANCE_MEASURE.lower()}.parquet'))\n",
    "# print(\"Adjacency matrix created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbc46611d3dfd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:48:14.639824Z",
     "start_time": "2024-07-23T11:47:48.104331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Script execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "regions_gdf = load_data()\n",
    "\n",
    "adjacency_file_path = os.path.join(CHECKPOINT_DIR, f'adjacency_matrix_res{RESOLUTION}_{DISTANCE_MEASURE.lower()}.parquet')\n",
    "\n",
    "# Try to import adjacency graph, otherwise create it and export it\n",
    "if os.path.exists(adjacency_file_path):\n",
    "    print(\"Loading existing adjacency graph...\")\n",
    "    adjacency_df = pd.read_parquet(adjacency_file_path)\n",
    "\n",
    "    # Convert the adjacency matrix to an edge list using melt\n",
    "    edge_list = adjacency_df.reset_index().melt(id_vars='region_id', var_name='target', value_name='weight')\n",
    "    edge_list.columns = ['source', 'target', 'weight']\n",
    "\n",
    "    # Remove edges with NaN or zero weights\n",
    "    edge_list = edge_list.dropna().query('weight != 0')\n",
    "\n",
    "    # Create a new directed graph\n",
    "    adjacency_graph = nx.DiGraph()\n",
    "    adjacency_graph.add_weighted_edges_from(edge_list.itertuples(index=False))\n",
    "\n",
    "    print(f\"Loaded graph with {adjacency_graph.number_of_nodes()} nodes and {adjacency_graph.number_of_edges()} edges.\")\n",
    "else:\n",
    "    print(\"Creating new adjacency graph...\")\n",
    "    if DISTANCE_MEASURE == \"Location_based_accessibility\":\n",
    "        G_drive = load_and_prepare_driving_graph()\n",
    "        adjacency_matrix = create_adjacency_graph(regions_gdf, distance=ADJACENCY_DISTANCE, distance_measure=DISTANCE_MEASURE, G_drive=G_drive)\n",
    "\n",
    "        # Export adjacency matrix\n",
    "        adjacency_matrix.to_parquet(os.path.join(CHECKPOINT_DIR, f'adjacency_matrix_res{RESOLUTION}_{DISTANCE_MEASURE.lower()}.parquet'))\n",
    "    elif DISTANCE_MEASURE == \"Euclidean\":\n",
    "        adjacency_graph = create_adjacency_graph(regions_gdf, distance=ADJACENCY_DISTANCE, distance_measure=DISTANCE_MEASURE, num_workers=NUM_WORKERS)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid DISTANCE_MEASURE: {DISTANCE_MEASURE}\")\n",
    "\n",
    "dataset = Urban2VecDataset(regions_gdf, adjacency_matrix, num_ants=NUM_ANTS, walk_length=WALK_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "num_regions = dataset.__len__()\n",
    "model = Urban2VecModel(num_regions, EMBEDDING_DIM).to(device)\n",
    "\n",
    "try:\n",
    "    pretrained_embeddings = load_pretrained_embeddings(dataset.regions, PRETRAINED_EMBEDDING_FILE)\n",
    "    model.region_embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    print(\"Loaded and PCA-reduced/used as is pre-trained region embeddings.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Pre-trained embeddings not found. Initializing randomly.\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CircleLoss()\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "try:\n",
    "    model, loss_values = train_urban2vec(model, dataloader, optimizer, criterion, device, NUM_EPOCHS)\n",
    "except RuntimeError as e:\n",
    "    print(f\"RuntimeError during training: {e}\")\n",
    "    print(\"Consider setting CUDA_LAUNCH_BLOCKING=1 for more detailed error messages.\")\n",
    "\n",
    "print(\"Saving final embeddings...\")\n",
    "region_embeddings = model.region_embedding.weight.detach().cpu().numpy()\n",
    "pd.DataFrame(region_embeddings, index=dataset.regions).to_csv(os.path.join(CHECKPOINT_DIR, f'step3_region_embeddings_{DISTANCE_MEASURE.lower()}.csv'))\n",
    "\n",
    "print(\"Training completed and embeddings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45519706c02af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(loss_values)\n",
    "plt.plot(pd.Series(loss_values).rolling(100).mean())\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Circle Loss\")\n",
    "plt.show()\n",
    "plt.xlabel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a5d73805d54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ensure the regions_gdf and embeddings_aerial, embedding_df are loaded correctly\n",
    "# Assuming regions_gdf['afw', 'fys', 'onv', 'soc', 'vrz', 'won'] contains the scores\n",
    "\n",
    "# Get the Leefbaarometer scores\n",
    "scores_df = regions_gdf[['afw', 'fys', 'onv', 'soc', 'vrz', 'won']]\n",
    "\n",
    "# Initialize lists to store R-squared values\n",
    "r2_scores_step1 = []\n",
    "r2_scores_step2 = []\n",
    "r2_scores_step3 = []\n",
    "score_names = scores_df.columns\n",
    "\n",
    "# Get the embeddings as DataFrames with region_id as index\n",
    "embeddings_step1 = pd.read_csv(CHECKPOINT_DIR + '/step1_region_embeddings.csv')\n",
    "embeddings_step2 = pd.read_csv(CHECKPOINT_DIR + '/step2_region_embeddings.csv')\n",
    "embeddings_step3 = pd.DataFrame(region_embeddings, index=dataset.regions)\n",
    "\n",
    "# set index column to 'Unnamed: 0' and rename as 'region_id'\n",
    "embeddings_step1 = embeddings_step1.set_index('Unnamed: 0').rename_axis('region_id')\n",
    "embeddings_step2 = embeddings_step2.set_index('Unnamed: 0').rename_axis('region_id')\n",
    "embeddings_step3 = embeddings_step3.rename_axis('region_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e94b68cc83b849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T10:40:43.658997Z",
     "start_time": "2024-07-23T10:40:43.658997Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_step3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef9aefcc5d66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure same dimensionality with PCA\n",
    "pca = PCA(n_components=30)\n",
    "embeddings_reduced_step1 = pca.fit_transform(embeddings_step1)\n",
    "embeddings_reduced_step2 = pca.fit_transform(embeddings_step2)\n",
    "embeddings_reduced_step3_euclidean = pca.fit_transform(embeddings_step3)\n",
    "\n",
    "# Loop over each score\n",
    "for score_name in score_names:\n",
    "    scores = scores_df[score_name]\n",
    "\n",
    "    # Fit the model for step 1 (using reduced embeddings)\n",
    "    model_step1 = LinearRegression().fit(embeddings_reduced_step1, scores)\n",
    "    r2_step1 = r2_score(scores, model_step1.predict(embeddings_reduced_step1))\n",
    "    r2_scores_step1.append(r2_step1)\n",
    "\n",
    "    # Fit the model for step 2 (using reduced embeddings)\n",
    "    model_step2 = LinearRegression().fit(embeddings_reduced_step2, scores)\n",
    "    r2_step2 = r2_score(scores, model_step2.predict(embeddings_reduced_step2))\n",
    "    r2_scores_step2.append(r2_step2)\n",
    "\n",
    "    # Fit the model for step 3 (using reduced embeddings)\n",
    "    model_step3 = LinearRegression().fit(embeddings_reduced_step3, scores)\n",
    "    r2_step3 = r2_score(scores, model_step3.predict(embeddings_reduced_step3))\n",
    "    r2_scores_step3.append(r2_step3)\n",
    "\n",
    "# Plotting the R-squared values\n",
    "x = np.arange(len(score_names))\n",
    "width = 0.25  # Reduced width for better spacing\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar1 = ax.bar(x - width, r2_scores_step1, width, label='Step 1')\n",
    "bar2 = ax.bar(x, r2_scores_step2, width, label='Step 2')  # Shifted to center\n",
    "bar3 = ax.bar(x + width, r2_scores_step3, width, label='Step 3')\n",
    "\n",
    "# Adding labels and titles\n",
    "ax.set_xlabel('Leefbaarometer Scores')\n",
    "ax.set_ylabel('R-squared Value')\n",
    "ax.set_title('R-squared Values of Embeddings Predicting Leefbaarometer Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(score_names)\n",
    "ax.legend()\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dc21505b9f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Plotting import pca_plot, cluster_agglomerative_plot, cluster_kmeans_plot\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# cluster_agglomerative_plot(embeddings_step3, regions_gdf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16abd60de1920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise location based accessibility\n",
    "# Load the adjacency matrix\n",
    "adjacency_df = pd.read_parquet(os.path.join(CHECKPOINT_DIR, f'adjacency_matrix_res{RESOLUTION}_{DISTANCE_MEASURE.lower()}.parquet'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
