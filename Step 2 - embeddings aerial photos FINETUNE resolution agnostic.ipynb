{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669408ec30a29be3",
   "metadata": {},
   "source": [
    "This script is used to finetune the image encoder model using the Circle Loss. The model is trained on a triplet dataset where each triplet consists of an anchor, positive, and negative image. The model is trained to minimize the distance between the anchor and positive images while maximizing the distance between the anchor and negative images. The model is trained using the Circle Loss function which is a variant of the triplet loss function. The model is trained for a fixed number of epochs and the embeddings are generated for the central regions. The embeddings are saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:34:06.344269Z",
     "start_time": "2024-07-18T08:33:59.006128Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import convnext_large, ConvNeXt_Large_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "import random\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461026a96acda201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:34:06.347643Z",
     "start_time": "2024-07-18T08:34:06.345272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the resolution here (9 or 10)\n",
    "RESOLUTION = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28842eb230f2795f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:34:06.357727Z",
     "start_time": "2024-07-18T08:34:06.349150Z"
    }
   },
   "outputs": [],
   "source": [
    "class BufferedH3TripletDataset(Dataset):\n",
    "    def __init__(self, regions_buffered_gdf, image_dir):\n",
    "        self.regions_buffered_gdf = regions_buffered_gdf\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.neighborhood = H3Neighbourhood(regions_buffered_gdf)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regions_buffered_gdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_id = self.regions_buffered_gdf.index[idx]\n",
    "        \n",
    "        # Sample k-ring (inside positive - outside negative) hard batch mining\n",
    "        if RESOLUTION == 9:\n",
    "            positive_ring = random.choice([1, 2])\n",
    "            negative_ring = random.choice([3, 4])\n",
    "        elif RESOLUTION == 10:\n",
    "            positive_ring = random.randint(1, 4)\n",
    "            negative_ring = random.randint(5, 8)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported resolution\")\n",
    "        \n",
    "        positive_neighbors = self.neighborhood.get_neighbours_at_distance(anchor_id, positive_ring)\n",
    "        negative_neighbors = self.neighborhood.get_neighbours_at_distance(anchor_id, negative_ring)\n",
    "        \n",
    "        # Pick a random region_id from the sampled k-ring\n",
    "        positive_id = random.choice(list(positive_neighbors)) if positive_neighbors else anchor_id\n",
    "        negative_id = random.choice(list(negative_neighbors)) if negative_neighbors else anchor_id\n",
    "\n",
    "        return (self.load_image(anchor_id), self.load_image(positive_id), self.load_image(negative_id)), (anchor_id, positive_id, negative_id)\n",
    "\n",
    "    def load_image(self, region_id):\n",
    "        image_path = os.path.join(self.image_dir, f\"{region_id}.jpg\")\n",
    "        if os.path.exists(image_path):\n",
    "            return self.transform(Image.open(image_path).convert('RGB'))\n",
    "        return torch.zeros(3, 224, 224)\n",
    "\n",
    "class FineTunedConvNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FineTunedConvNeXt, self).__init__()\n",
    "        # Load the pretrained ConvNeXt model\n",
    "        self.convnext = convnext_large(weights=ConvNeXt_Large_Weights.DEFAULT)\n",
    "        # Replace the classifier layer with an identity layer to get embeddings\n",
    "        self.convnext.classifier = nn.Identity()\n",
    "        # Verify the output size of the model\n",
    "        print(f\"ConvNeXt output size: {self.convnext(torch.randn(1, 3, 224, 224)).shape[1]}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass input through ConvNeXt to get embeddings\n",
    "        embeddings = self.convnext(x)\n",
    "        # Flatten the output if necessary\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        return embeddings\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m=0.25, gamma=256):\n",
    "        super().__init__()\n",
    "        self.m, self.gamma = m, gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "    def forward(self, sp, sn):\n",
    "        ap = torch.clamp_min(-sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "        delta_p, delta_n = 1 - self.m, self.m\n",
    "        logit_p = -ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "        return self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45751a89ef1a940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:34:06.364508Z",
     "start_time": "2024-07-18T08:34:06.358228Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device, epochs, checkpoint_dir, resume_epoch=0):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(resume_epoch, epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for (anchor_imgs, positive_imgs, negative_imgs), _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            anchor_imgs, positive_imgs, negative_imgs = anchor_imgs.to(device), positive_imgs.to(device), negative_imgs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            anchor_features, positive_features, negative_features = model(anchor_imgs), model(positive_imgs), model(negative_imgs)\n",
    "            sp, sn = (anchor_features * positive_features).sum(dim=1), (anchor_features * negative_features).sum(dim=1)\n",
    "            loss = criterion(sp, sn)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            wandb.log({\"batch_loss\": loss.item(), \"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        wandb.log({\"epoch\": epoch+1, \"average_loss\": avg_loss})\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, f'best_model_{RESOLUTION}.pth'))\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'final_model_{RESOLUTION}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1eacb05a3fe8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:34:06.371031Z",
     "start_time": "2024-07-18T08:34:06.365510Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(model, regions_gdf, image_dir, device, batch_size=64):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = RegionDataset(regions_gdf, image_dir, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    embeddings = {}\n",
    "    missing_regions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, region_ids in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            for feature, region_id in zip(features, region_ids):\n",
    "                if torch.all(feature == 0):\n",
    "                    missing_regions.append(region_id)\n",
    "                embeddings[region_id] = feature.cpu().numpy()\n",
    "\n",
    "    print(f\"Number of missing regions: {len(missing_regions)}\")\n",
    "    return pd.DataFrame.from_dict(embeddings, orient='index'), missing_regions\n",
    "\n",
    "class RegionDataset(Dataset):\n",
    "    def __init__(self, regions_gdf, image_dir, transform):\n",
    "        self.regions_gdf, self.image_dir, self.transform = regions_gdf, image_dir, transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regions_gdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        region_id = self.regions_gdf.index[idx]\n",
    "        try:\n",
    "            image = Image.open(os.path.join(self.image_dir, f\"{region_id}.jpg\")).convert('RGB')\n",
    "            return self.transform(image), region_id\n",
    "        except FileNotFoundError:\n",
    "            # Return a zero tensor instead of None\n",
    "            return torch.zeros(3, 224, 224), region_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff4b70822f1aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T18:19:19.968848Z",
     "start_time": "2024-07-18T08:34:06.372032Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=\"Urban_Representation_Learning\", config={\n",
    "        \"learning_rate\": 1e-5,  # Adjusted for Adam optimizer\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 16,\n",
    "        \"resolution\": RESOLUTION,\n",
    "        \"weight_decay\": 1e-4,  # Adjusted for Adam optimizer\n",
    "    })\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    regions_gdf = gpd.read_file(f\"selected_regions_{RESOLUTION}.geojson\").set_index(\"region_id\")\n",
    "    regions_buffered_gdf = gpd.read_file(f\"selected_regions_buffered_{RESOLUTION}.geojson\").set_index(\"region_id\")\n",
    "    image_dir = fr\"D:\\tu delft\\Afstuderen\\aerial_images_{RESOLUTION}\"\n",
    "\n",
    "    dataset = BufferedH3TripletDataset(regions_buffered_gdf, image_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "\n",
    "    model = FineTunedConvNeXt().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
    "    criterion = CircleLoss()\n",
    "    \n",
    "    checkpoint_dir = fr\"D:\\tu delft\\Afstuderen\\Phase 6 Experiments\\checkpoints_res{RESOLUTION}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    resume_epoch = 0\n",
    "\n",
    "    best_model_path = os.path.join(checkpoint_dir, f'best_model_{RESOLUTION}.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        resume_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {resume_epoch}\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, dataloader, optimizer, criterion, device, wandb.config.epochs, checkpoint_dir, resume_epoch)\n",
    "\n",
    "    # After calling generate_embeddings\n",
    "    embeddings_df, missing_regions = generate_embeddings(model, regions_gdf, image_dir, device)\n",
    "    \n",
    "    print(f\"Number of missing regions: {len(missing_regions)}\")\n",
    "    \n",
    "    # Local averaging for missing region_ids\n",
    "    neighborhood = H3Neighbourhood(regions_buffered_gdf)\n",
    "    for region_id in missing_regions:\n",
    "        neighbors = neighborhood.get_neighbours_at_distance(region_id, 1)\n",
    "        neighbor_embeddings = embeddings_df.loc[embeddings_df.index.isin(neighbors)]\n",
    "        if not neighbor_embeddings.empty:\n",
    "            embeddings_df.loc[region_id] = neighbor_embeddings.mean()\n",
    "    \n",
    "    # Save embeddings\n",
    "    output_dir = r\"D:\\tu delft\\Afstuderen\\Phase 6 Experiments\\embeddings\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f\"learned_finetune_circle_h3_res_{RESOLUTION}.csv\")\n",
    "    embeddings_df.to_csv(output_file)\n",
    "    print(f\"Embeddings saved to {output_file}\")\n",
    "    \n",
    "    # Print information about remaining missing regions\n",
    "    remaining_missing = set(missing_regions) - set(embeddings_df.index)\n",
    "    print(f\"Number of regions still missing after local averaging: {len(remaining_missing)}\")\n",
    "    if remaining_missing:\n",
    "        print(\"These regions have no neighbors with embeddings:\")\n",
    "        print(remaining_missing)\n",
    "    \n",
    "    model.eval()\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324853c5b3d2235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T18:28:45.705647Z",
     "start_time": "2024-07-18T18:27:17.544048Z"
    }
   },
   "outputs": [],
   "source": [
    "from Plotting import pca_plot, cluster_agglomerative_plot, cluster_kmeans_plot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "embeddings_reduced_df = pca.fit_transform(embeddings_df)\n",
    "# print variance explained\n",
    "print(f\"Variance explained: {pca.explained_variance_ratio_.sum()*100}%\")\n",
    "# ensure the index is preserved\n",
    "embeddings_reduced_df = pd.DataFrame(embeddings_reduced_df, index=embeddings_df.index)\n",
    "cluster_agglomerative_plot(embeddings_reduced_df, regions_gdf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f77583c5527af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T18:19:19.973349Z",
     "start_time": "2024-07-18T18:19:19.969850Z"
    }
   },
   "outputs": [],
   "source": [
    "# # After calling generate_embeddings\n",
    "# embeddings_df, missing_regions = generate_embeddings(model, regions_gdf, image_dir, device)\n",
    "# \n",
    "# print(f\"Number of missing regions: {len(missing_regions)}\")\n",
    "# \n",
    "# # Local averaging for missing region_ids\n",
    "# neighborhood = H3Neighbourhood(regions_buffered_gdf)\n",
    "# for region_id in missing_regions:\n",
    "#     neighbors = neighborhood.get_neighbours_at_distance(region_id, 1)\n",
    "#     neighbor_embeddings = embeddings_df.loc[embeddings_df.index.isin(neighbors)]\n",
    "#     if not neighbor_embeddings.empty:\n",
    "#         embeddings_df.loc[region_id] = neighbor_embeddings.mean()\n",
    "# \n",
    "# # Save embeddings\n",
    "# output_dir = r\"D:\\tu delft\\Afstuderen\\Phase 6 Experiments\\embeddings\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_file = os.path.join(output_dir, f\"learned_finetune_circle_h3_res_{RESOLUTION}.csv\")\n",
    "# embeddings_df.to_csv(output_file)\n",
    "# print(f\"Embeddings saved to {output_file}\")\n",
    "# \n",
    "# # Print information about remaining missing regions\n",
    "# remaining_missing = set(missing_regions) - set(embeddings_df.index)\n",
    "# print(f\"Number of regions still missing after local averaging: {len(remaining_missing)}\")\n",
    "# if remaining_missing:\n",
    "#     print(\"These regions have no neighbors with embeddings:\")\n",
    "#     print(remaining_missing)\n",
    "# \n",
    "# model.eval()\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Total number of parameters: {total_params:,}\")\n",
    "# \n",
    "# wandb.finish()\n",
    "# print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bf374dd064e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T18:19:19.977463Z",
     "start_time": "2024-07-18T18:19:19.974351Z"
    }
   },
   "outputs": [],
   "source": [
    "# # plotting embeddings for test\n",
    "# from Plotting import pca_plot, cluster_agglomerative_plot, cluster_kmeans_plot\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# embeddings_reduced_df = pca.fit_transform(embeddings_df)\n",
    "# # print variance explained\n",
    "# print(f\"Variance explained: {pca.explained_variance_ratio_.sum()*100}%\")\n",
    "# # ensure the index is preserved\n",
    "# embeddings_reduced_df = pd.DataFrame(embeddings_reduced_df, index=embeddings_df.index)\n",
    "# cluster_agglomerative_plot(embeddings_reduced_df, regions_gdf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf2b278f52ec98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T18:19:19.980676Z",
     "start_time": "2024-07-18T18:19:19.977965Z"
    }
   },
   "outputs": [],
   "source": [
    "# # get r squared value of dim reduced embeddings to leefbaarometer scores in region_gdf (regions_gdf['afw'])\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import r2_score\n",
    "# import numpy as np\n",
    "# # get the scores\n",
    "# scores = regions_gdf['afw']\n",
    "# # get the embeddings\n",
    "# embeddings = embeddings_df.loc[regions_gdf.index]\n",
    "# # fit the model\n",
    "# model = LinearRegression().fit(embeddings, scores)\n",
    "# # get the r squared value\n",
    "# r2 = r2_score(scores, model.predict(embeddings))\n",
    "# print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc90d64fca00ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T18:19:19.983Z",
     "start_time": "2024-07-18T18:19:19.981178Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
