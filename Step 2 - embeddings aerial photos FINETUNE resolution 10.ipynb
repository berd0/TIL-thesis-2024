{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669408ec30a29be3",
   "metadata": {},
   "source": [
    "This script is used to finetune the image encoder model using the Circle Loss. The model is trained on a triplet dataset where each triplet consists of an anchor, positive, and negative image. The model is trained to minimize the distance between the anchor and positive images while maximizing the distance between the anchor and negative images. The model is trained using the Circle Loss function which is a variant of the triplet loss function. The model is trained for a fixed number of epochs and the embeddings are generated for the central regions. The embeddings are saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:41:57.193497Z",
     "start_time": "2024-07-08T10:41:53.318528Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import convnext_large, ConvNeXt_Large_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "import random\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28842eb230f2795f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:41:57.202850Z",
     "start_time": "2024-07-08T10:41:57.194498Z"
    }
   },
   "outputs": [],
   "source": [
    "class BufferedH3TripletDataset(Dataset):\n",
    "    def __init__(self, regions_buffered_gdf, image_dir):\n",
    "        self.regions_buffered_gdf = regions_buffered_gdf\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.neighborhood = H3Neighbourhood(regions_buffered_gdf)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regions_buffered_gdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_id = self.regions_buffered_gdf.index[idx]\n",
    "    \n",
    "        positive_ring = random.randint(1, 8)  # Select a random ring distance for positive neighbors\n",
    "        negative_ring = random.randint(9, 16)  # Select a random ring distance for negative neighbors\n",
    "    \n",
    "        positive_neighbors = self.neighborhood.get_neighbours_at_distance(anchor_id, positive_ring)\n",
    "        negative_neighbors = self.neighborhood.get_neighbours_at_distance(anchor_id, negative_ring)\n",
    "    \n",
    "        # Select a random positive neighbor if available, else use anchor_id\n",
    "        positive_id = random.choice(list(positive_neighbors)) if positive_neighbors else anchor_id\n",
    "        # Select a random negative neighbor if available, else use anchor_id\n",
    "        negative_id = random.choice(list(negative_neighbors)) if negative_neighbors else anchor_id\n",
    "    \n",
    "        anchor_image = self.load_image(anchor_id)\n",
    "        positive_image = self.load_image(positive_id)\n",
    "        negative_image = self.load_image(negative_id)\n",
    "    \n",
    "        return (anchor_image, positive_image, negative_image), (anchor_id, positive_id, negative_id)\n",
    "\n",
    "    def load_image(self, region_id):\n",
    "        image_path = os.path.join(self.image_dir, f\"{region_id}.jpg\")\n",
    "        if os.path.exists(image_path):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            return self.transform(image)\n",
    "        else:\n",
    "            return torch.zeros(3, 224, 224)  # Return a tensor filled with zeros if the image is not found\n",
    "\n",
    "class FineTunedConvNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convnext = convnext_large(weights=ConvNeXt_Large_Weights.DEFAULT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.convnext(x)\n",
    "        return features.view(features.size(0), -1)  # Flatten the features\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m=0.25, gamma=256):\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.m = m\n",
    "        self.gamma = gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "    def forward(self, sp, sn):\n",
    "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "\n",
    "        delta_p = 1 - self.m\n",
    "        delta_n = self.m\n",
    "\n",
    "        logit_p = - ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "\n",
    "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45751a89ef1a940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:41:57.209565Z",
     "start_time": "2024-07-08T10:41:57.203852Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device, epochs, checkpoint_dir, resume_epoch=0):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(resume_epoch, epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for (anchor_imgs, positive_imgs, negative_imgs), _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            anchor_imgs, positive_imgs, negative_imgs = anchor_imgs.to(device), positive_imgs.to(device), negative_imgs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            anchor_features = model(anchor_imgs)\n",
    "            positive_features = model(positive_imgs)\n",
    "            negative_features = model(negative_imgs)\n",
    "\n",
    "            sp = (anchor_features * positive_features).sum(dim=1)\n",
    "            sn = (anchor_features * negative_features).sum(dim=1)\n",
    "\n",
    "            loss = criterion(sp, sn)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            wandb.log({\"batch_loss\": loss.item(), \"learning_rate\": current_lr})\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        wandb.log({\"epoch\": epoch+1, \"average_loss\": avg_loss})\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, 'best_model_10.pth'))\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'final_model_10.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1eacb05a3fe8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T15:41:22.488008Z",
     "start_time": "2024-07-08T15:41:22.481076Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(model, regions_gdf, image_dir, device, batch_size=32):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    embeddings = {}\n",
    "    dataloader = DataLoader(\n",
    "        RegionDataset(regions_gdf, image_dir, transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, region_ids in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            for feature, region_id in zip(features, region_ids):\n",
    "                embeddings[region_id] = feature.cpu().numpy()\n",
    "\n",
    "    return pd.DataFrame.from_dict(embeddings, orient='index')\n",
    "\n",
    "class RegionDataset(Dataset):\n",
    "    def __init__(self, regions_gdf, image_dir, transform):\n",
    "        self.regions_gdf = regions_gdf\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regions_gdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        region_id = self.regions_gdf.index[idx]\n",
    "        image_path = os.path.join(self.image_dir, f\"{region_id}.jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return self.transform(image), region_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff4b70822f1aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T15:30:26.719766Z",
     "start_time": "2024-07-08T10:41:57.216693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use this function in your main script\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=\"Urban_Representation_Learning\", config={\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 16,\n",
    "        \"resolution\": 10,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    })\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    regions_gdf = gpd.read_file(\"selected_regions_10.geojson\").set_index(\"region_id\")\n",
    "    regions_buffered_gdf = gpd.read_file(\"selected_regions_buffered_10.geojson\").set_index(\"region_id\")\n",
    "    image_dir = r\"D:\\tu delft\\Afstuderen\\aerial_images_10\"\n",
    "\n",
    "    dataset = BufferedH3TripletDataset(regions_buffered_gdf, image_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "\n",
    "    model = FineTunedConvNeXt().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
    "    criterion = CircleLoss()\n",
    "\n",
    "    checkpoint_dir = r\"/Phase 6 Experiments/checkpoints_res10\"\n",
    "    resume_epoch = 0\n",
    "\n",
    "    # Check if there's a checkpoint to resume from\n",
    "    checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_')])\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = checkpoints[-1]\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, latest_checkpoint))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        resume_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {resume_epoch}\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, dataloader, optimizer, criterion, device, wandb.config.epochs, checkpoint_dir, resume_epoch)\n",
    "\n",
    "    print(\"Generating embeddings for central regions...\")\n",
    "    embeddings_df = generate_embeddings(model, regions_gdf, image_dir, device)\n",
    "\n",
    "    output_dir = r\"D:\\tu delft\\Afstuderen\\Phase 6 Experiments\\embeddings\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f\"learned_finetune_circle_h3_res_{wandb.config.resolution}.csv\")\n",
    "    embeddings_df.to_csv(output_file)\n",
    "    print(f\"Embeddings saved to {output_file}\")\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c3b05ac236936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
