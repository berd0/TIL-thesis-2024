{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T10:18:17.191224Z",
     "start_time": "2024-07-25T10:18:14.898560Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbe1dc30ccb69c",
   "metadata": {},
   "source": [
    "This code is a sped up version, made by claude sonnet 3.5, which uses poi, roadnetwork and gtfs embeddings to create a new embedding for each region. The new embedding is created by taking the exponential weighted average of the region's embedding and its neighbors' embeddings. The code is optimized to run on GPU and uses the H3 hexagonal grid to find neighbors of each region.\n",
    "\n",
    "It is parallel for both k-ring and datasources. It also has multiple options in the execution loop to choose specific data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f09fcb60fae9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T10:18:17.210800Z",
     "start_time": "2024-07-25T10:18:17.192225Z"
    }
   },
   "outputs": [],
   "source": [
    "class WithinRingNN(nn.Module):\n",
    "    def __init__(self, full_input_dim, hidden_dim):\n",
    "        super(WithinRingNN, self).__init__()\n",
    "        self.bn_input = nn.BatchNorm1d(full_input_dim)\n",
    "        self.fc1 = nn.Linear(full_input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn_input(x)\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class BetweenRingNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(BetweenRingNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RingAggregationNN(nn.Module):\n",
    "    def __init__(self, full_input_dim, hidden_dim, output_dim, k, weight_type='exponential_e'):\n",
    "        super(RingAggregationNN, self).__init__()\n",
    "        self.k = k\n",
    "        self.weight_type = weight_type\n",
    "        self.within_ring_nn = WithinRingNN(full_input_dim, hidden_dim)\n",
    "        self.between_ring_nn = BetweenRingNN(hidden_dim, output_dim)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        batch_size, num_rings, max_neighbors, _ = embeddings.size()\n",
    "        transformed_and_pooled_embeddings = []\n",
    "\n",
    "        for i in range(num_rings):\n",
    "            ring_embeddings = embeddings[:, i, :, :].reshape(batch_size * max_neighbors, -1)\n",
    "            transformed_embeddings = self.within_ring_nn(ring_embeddings)\n",
    "            pooled_embeddings = transformed_embeddings.view(batch_size, max_neighbors, -1).mean(dim=1)\n",
    "            transformed_and_pooled_embeddings.append(pooled_embeddings)\n",
    "\n",
    "        ring_means = torch.stack(transformed_and_pooled_embeddings, dim=1)\n",
    "        transformed_ring_means = []\n",
    "\n",
    "        for i in range(num_rings):\n",
    "            transformed_mean = self.between_ring_nn(ring_means[:, i, :])\n",
    "            transformed_ring_means.append(transformed_mean)\n",
    "\n",
    "        # Calculate weights\n",
    "        weights = torch.zeros(num_rings, device=self.device)\n",
    "        for i in range(num_rings):\n",
    "            if self.weight_type == 'exponential_e':\n",
    "                weights[i] = torch.exp(torch.tensor(-i, dtype=torch.float32))\n",
    "            elif self.weight_type == 'logarithm':\n",
    "                weights[i] = 1 / torch.log2(torch.tensor(i + 2, dtype=torch.float32))\n",
    "            elif self.weight_type == 'linear':\n",
    "                weights[i] = 1 - i / (num_rings - 1)\n",
    "            elif self.weight_type == 'flat':\n",
    "                weights[i] = 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported weight_type: {self.weight_type}\")\n",
    "\n",
    "        # Normalize weights\n",
    "        weights = weights / weights.sum()\n",
    "\n",
    "        # Apply weights and sum\n",
    "        weighted_sum = torch.zeros_like(transformed_ring_means[0])\n",
    "        for i in range(num_rings):\n",
    "            weighted_sum += weights[i] * transformed_ring_means[i]\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m=0.15, gamma=256):  # maybe reduce m to 0.15? See circle loss paper, 0.25 was best but had hard dropoff so better play safe?\n",
    "        super().__init__()\n",
    "        self.m, self.gamma = m, gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "    def forward(self, sp, sn):\n",
    "        ap = torch.clamp_min(-sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "        delta_p, delta_n = 1 - self.m, self.m\n",
    "        logit_p = -ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "        return self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, region_ids, positive_lookup_table, k, embeddings_concatenated):\n",
    "        self.region_ids = region_ids\n",
    "        self.positive_lookup_table = positive_lookup_table\n",
    "        self.k = k\n",
    "        self.embeddings_concatenated = embeddings_concatenated\n",
    "        self.neighborhood = H3Neighbourhood(embeddings_concatenated)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.region_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_id = self.region_ids[idx]\n",
    "        possible_positives = self.positive_lookup_table.loc[anchor_id].index[self.positive_lookup_table.loc[anchor_id] == 1].tolist()\n",
    "        possible_negatives = self.positive_lookup_table.loc[anchor_id].index[self.positive_lookup_table.loc[anchor_id] == 0].tolist()\n",
    "\n",
    "        if not possible_positives or not possible_negatives:\n",
    "            return self.__getitem__((idx + 1) % len(self.region_ids))\n",
    "\n",
    "        positive_id = np.random.choice(possible_positives)\n",
    "        negative_id = np.random.choice(possible_negatives)\n",
    "\n",
    "        anchor_tensor = self.get_embeddings_for_region_tensor(anchor_id)\n",
    "        positive_tensor = self.get_embeddings_for_region_tensor(positive_id)\n",
    "        negative_tensor = self.get_embeddings_for_region_tensor(negative_id)\n",
    "\n",
    "        return anchor_tensor, positive_tensor, negative_tensor\n",
    "\n",
    "    def get_embeddings_for_region_tensor(self, region_id):\n",
    "        group_embeddings = []\n",
    "        max_neighbors = 6 * self.k\n",
    "\n",
    "        for i in range(self.k + 1):\n",
    "            if i == 0:\n",
    "                embeddings = torch.tensor(self.embeddings_concatenated.loc[[region_id]].values)\n",
    "            else:\n",
    "                neighbor_ids = self.get_neighbors(region_id, i)\n",
    "                if len(neighbor_ids) > 0:\n",
    "                    embeddings = torch.tensor(self.embeddings_concatenated.loc[neighbor_ids].values)\n",
    "                else:\n",
    "                    embeddings = torch.zeros((0, self.embeddings_concatenated.shape[1]))\n",
    "\n",
    "            padded_embeddings = F.pad(embeddings, (0, 0, 0, max(max_neighbors - embeddings.shape[0], 0)))\n",
    "            group_embeddings.append(padded_embeddings)\n",
    "\n",
    "        padded_group_embeddings = torch.stack([F.pad(x, (0, 0, 0, max_neighbors - x.size(0))) for x in group_embeddings])\n",
    "        return padded_group_embeddings\n",
    "\n",
    "    def get_neighbors(self, region_id, k):\n",
    "        return list(self.neighborhood.get_neighbours_at_distance(str(region_id), k))\n",
    "\n",
    "class RegionEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, region_ids, k, embeddings_concatenated, padding_size=0):\n",
    "        self.region_ids = region_ids\n",
    "        self.k = k\n",
    "        self.embeddings_concatenated = embeddings_concatenated\n",
    "        self.padding_size = padding_size\n",
    "        self.neighborhood = H3Neighbourhood(embeddings_concatenated)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.region_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        region_id = self.region_ids[idx]\n",
    "        embeddings_tensor = self.get_embeddings_for_region_tensor(region_id)\n",
    "        return embeddings_tensor\n",
    "\n",
    "    def get_embeddings_for_region_tensor(self, region_id):\n",
    "        group_embeddings = []\n",
    "        max_neighbors = 6 * self.k\n",
    "\n",
    "        for i in range(self.k + 1):\n",
    "            if i == 0:\n",
    "                embeddings = torch.tensor(self.embeddings_concatenated.loc[[region_id]].values)\n",
    "            else:\n",
    "                neighbor_ids = self.get_neighbors(region_id, i)\n",
    "                if len(neighbor_ids) > 0:\n",
    "                    embeddings = torch.tensor(self.embeddings_concatenated.loc[neighbor_ids].values)\n",
    "                else:\n",
    "                    embeddings = torch.zeros((0, self.embeddings_concatenated.shape[1]))\n",
    "\n",
    "            # Add padding if necessary\n",
    "            if self.padding_size > 0:\n",
    "                padding = torch.zeros(embeddings.shape[0], self.padding_size)\n",
    "                embeddings = torch.cat([embeddings, padding], dim=1)\n",
    "\n",
    "            padded_embeddings = F.pad(embeddings, (0, 0, 0, max(max_neighbors - embeddings.shape[0], 0)))\n",
    "            group_embeddings.append(padded_embeddings)\n",
    "\n",
    "        padded_group_embeddings = torch.stack([F.pad(x, (0, 0, 0, max_neighbors - x.size(0))) for x in group_embeddings])\n",
    "        return padded_group_embeddings\n",
    "\n",
    "    def get_neighbors(self, region_id, k):\n",
    "        return list(self.neighborhood.get_neighbours_at_distance(str(region_id), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b9c12cafbfd09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T10:18:17.255938Z",
     "start_time": "2024-07-25T10:18:17.211808Z"
    }
   },
   "outputs": [],
   "source": [
    "class LearntAggregationExperiment:\n",
    "    def __init__(self, resolution=9, use_euclidean=False, poi_embedding='geovex',\n",
    "                 use_finetuned_aerial=False, use_finetuned_streetview=False,\n",
    "                 image_pca_dim=None, k_values=[1,3,5]):\n",
    "        self.resolution = resolution\n",
    "        self.use_euclidean = use_euclidean\n",
    "        self.k_values = k_values\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.experiment_dir = self.create_experiment_directory()\n",
    "        self.image_pca_dim = image_pca_dim\n",
    "        self.poi_embedding = poi_embedding\n",
    "        self.verbose = True\n",
    "        self.use_finetuned_aerial = use_finetuned_aerial\n",
    "        self.use_finetuned_streetview = use_finetuned_streetview\n",
    "        self.load_data()\n",
    "        self.prepare_data()\n",
    "\n",
    "        self.target_columns = ['afw', 'vrz', 'fys', 'soc', 'onv', 'won']\n",
    "        self.target_names = {\n",
    "            'afw': 'Liveability',\n",
    "            'vrz': 'Amenities',\n",
    "            'fys': 'Physical Environment',\n",
    "            'soc': 'Social Cohesion',\n",
    "            'onv': 'Safety',\n",
    "            'won': 'Housing Stock'\n",
    "        }\n",
    "        self.colors = {\n",
    "            'afw': '#808080',  # Dark Grey for Liveability\n",
    "            'vrz': '#FF4500',  # Orange Red for Amenities\n",
    "            'fys': '#32CD32',  # Lime Green for Physical Environment\n",
    "            'soc': '#8A2BE2',  # Blue Violet for Social Cohesion\n",
    "            'onv': '#1E90FF',  # Dodger Blue for Safety\n",
    "            'won': '#FFA500'   # Orange for Housing Stock\n",
    "        }\n",
    "        self.markers = {\n",
    "            'exponential_e': 'o',\n",
    "            'linear': 's',\n",
    "            'flat': '^',\n",
    "            'logarithm': 'D'\n",
    "        }\n",
    "\n",
    "    def create_experiment_directory(self):\n",
    "        date_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "        base_dir = \"experiments\"\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "        run_number = 1\n",
    "        while True:\n",
    "            dir_name = f\"{date_str}_run{run_number:02d}_res{self.resolution}_learnt\"\n",
    "            full_path = os.path.join(base_dir, dir_name)\n",
    "            if not os.path.exists(full_path):\n",
    "                os.makedirs(full_path)\n",
    "                return full_path\n",
    "            run_number += 1\n",
    "\n",
    "    def load_data(self):\n",
    "        if self.verbose:\n",
    "            print(f\"Loading data for resolution {self.resolution}...\")\n",
    "        self.od_matrix_accessibilities = pd.read_csv('od_matrix_accessibilities_neighborhooddensity.csv', index_col=0)\n",
    "    \n",
    "        self.embeddings = {}\n",
    "    \n",
    "        def load_csv_with_index(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'region_id' in df.columns:\n",
    "                df.set_index('region_id', inplace=True)\n",
    "            else:\n",
    "                df.set_index(df.columns[0], inplace=True)\n",
    "                df.index.name = 'region_id'\n",
    "            return df\n",
    "    \n",
    "        # Load POI embeddings\n",
    "        poi_file = f'embeddings_POI_{self.poi_embedding}_{self.resolution}.csv'\n",
    "        self.embeddings['POI'] = load_csv_with_index(poi_file)\n",
    "    \n",
    "        # Common embeddings for both resolutions\n",
    "        common_embeddings = ['roadnetwork', 'GTFS']\n",
    "        for emb in common_embeddings:\n",
    "            self.embeddings[emb] = load_csv_with_index(f'embeddings_{emb}_{self.resolution}.csv')\n",
    "    \n",
    "        # Load aerial embeddings\n",
    "        aerial_suffix = '_finetune' if self.use_finetuned_aerial else ''\n",
    "        self.embeddings['aerial'] = load_csv_with_index(f'embeddings_aerial_{self.resolution}{aerial_suffix}.csv')\n",
    "    \n",
    "        # Resolution-specific embeddings\n",
    "        if self.resolution == 9:\n",
    "            streetview_suffix = '_finetune' if self.use_finetuned_streetview else ''\n",
    "            self.embeddings['streetview'] = load_csv_with_index(f'embeddings_streetview_mean_{self.resolution}{streetview_suffix}.csv')\n",
    "    \n",
    "        self.regions_buffered_gdf = gpd.read_file(f'selected_regions_buffered_{self.resolution}.geojson').set_index('region_id')\n",
    "        self.regions_gdf = gpd.read_file(f'selected_regions_{self.resolution}.geojson').set_index('region_id')\n",
    "    \n",
    "        if self.verbose:\n",
    "            print(\"Data loaded successfully.\")\n",
    "            print(f\"Loaded embeddings: {list(self.embeddings.keys())}\")\n",
    "\n",
    "    def apply_pca(self, embedding_df, name, n_components):\n",
    "        if self.verbose:\n",
    "            print(f\"Applying PCA to {name} embeddings...\")\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_result = pca.fit_transform(embedding_df)\n",
    "        pca_df = pd.DataFrame(pca_result, index=embedding_df.index,\n",
    "                              columns=[f\"{name}_pca_{i}\" for i in range(n_components)])\n",
    "\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_.sum()\n",
    "        if self.verbose:\n",
    "            print(f\"PCA on {name} embeddings: {explained_variance_ratio:.2%} of variance explained\")\n",
    "\n",
    "        return pca_df\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if self.verbose:\n",
    "            print(\"Preparing data...\")\n",
    "\n",
    "        image_embeddings = ['aerial', 'streetview']\n",
    "    \n",
    "        for key in self.embeddings:\n",
    "            if key in image_embeddings and self.image_pca_dim is not None:\n",
    "                self.embeddings[key] = self.apply_pca(self.embeddings[key], f'{key}_pca', self.image_pca_dim)\n",
    "            if key == 'streetview':\n",
    "                self.embeddings[key] = self.embeddings[key][~self.embeddings[key].index.duplicated(keep='first')]\n",
    "            self.embeddings[key] = self.embeddings[key].reindex(index=self.regions_buffered_gdf.index, fill_value=0)\n",
    "    \n",
    "        self.embeddings_concatenated = pd.concat(list(self.embeddings.values()), axis=1)\n",
    "        self.embeddings_concatenated = self.embeddings_concatenated.loc[self.regions_buffered_gdf.index]\n",
    "        self.embeddings_concatenated.fillna(0, inplace=True)\n",
    "    \n",
    "        self.input_dim = self.embeddings_concatenated.shape[1]\n",
    "        self.output_dim = min(emb.shape[1] for emb in self.embeddings.values())\n",
    "    \n",
    "        self.max_neighbors = 6 * max(self.k_values)\n",
    "    \n",
    "        self.input_dim = self.input_dim * (self.max_neighbors + 1)\n",
    "\n",
    "        if self.use_euclidean:\n",
    "            self.distance_matrix = self.calculate_euclidean_distances()\n",
    "            threshold = np.percentile(self.distance_matrix.values, 2)\n",
    "            # For Euclidean, we want distances BELOW the threshold\n",
    "            self.positive_lookup_table = (self.distance_matrix < threshold).astype(int)\n",
    "        else:\n",
    "            accessibility_stacked = self.od_matrix_accessibilities.stack()\n",
    "            threshold = accessibility_stacked.quantile(0.98)\n",
    "            # For accessibility, we want values ABOVE the threshold\n",
    "            self.positive_lookup_table = (self.od_matrix_accessibilities > threshold).astype(int)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Data prepared.\")\n",
    "\n",
    "    def calculate_euclidean_distances(self):\n",
    "        if self.verbose:\n",
    "            print(\"Calculating Euclidean distances...\")\n",
    "        centroids = self.regions_gdf.geometry.centroid\n",
    "        coords = np.column_stack((centroids.x, centroids.y))\n",
    "        distances = pdist(coords, metric='euclidean')\n",
    "        distance_matrix = pd.DataFrame(squareform(distances), index=self.regions_gdf.index, columns=self.regions_gdf.index)\n",
    "        return distance_matrix\n",
    "\n",
    "    def select_embeddings(self, data_sources):\n",
    "        if data_sources == 'all':\n",
    "            return self.embeddings_concatenated\n",
    "        elif isinstance(data_sources, str):\n",
    "            return self.embeddings[data_sources]\n",
    "        else:\n",
    "            return pd.concat([self.embeddings[source] for source in data_sources], axis=1)\n",
    "        \n",
    "    def train_multiple_models(self, k_values, weight_type, hidden_dim=96, num_epochs=1, batch_size=256, lr=0.0001):\n",
    "        models = {}\n",
    "        losses = {}\n",
    "        max_k = max(k_values)\n",
    "\n",
    "        full_input_dim = self.embeddings_concatenated.shape[1]\n",
    "        output_dim = min(emb.shape[1] for emb in self.embeddings.values())\n",
    "\n",
    "        for k in k_values:\n",
    "            models[k] = RingAggregationNN(full_input_dim, hidden_dim, output_dim, k, weight_type=weight_type).to(self.device)\n",
    "            losses[k] = []\n",
    "\n",
    "        dataset_train = TripletDataset(self.regions_gdf.index.values, self.positive_lookup_table, max_k, self.embeddings_concatenated)\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "        optimizers = {k: torch.optim.Adam(models[k].parameters(), lr=lr, weight_decay=1e-5) for k in k_values}\n",
    "        circle_loss = CircleLoss().to(self.device)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for model in models.values():\n",
    "                model.train()\n",
    "\n",
    "            batch_losses = {k: [] for k in k_values}\n",
    "\n",
    "            for anchor_batch, positive_batch, negative_batch in dataloader_train:\n",
    "                anchor_batch = anchor_batch.float().to(self.device)\n",
    "                positive_batch = positive_batch.float().to(self.device)\n",
    "                negative_batch = negative_batch.float().to(self.device)\n",
    "\n",
    "                for k in k_values:\n",
    "                    optimizers[k].zero_grad()\n",
    "\n",
    "                    anchor_output = models[k](anchor_batch[:, :k+1])\n",
    "                    positive_output = models[k](positive_batch[:, :k+1])\n",
    "                    negative_output = models[k](negative_batch[:, :k+1])\n",
    "\n",
    "                    # Calculate cosine similarities\n",
    "                    pos_sim = F.cosine_similarity(anchor_output, positive_output)\n",
    "                    neg_sim = F.cosine_similarity(anchor_output, negative_output)\n",
    "\n",
    "                    # Apply CircleLoss\n",
    "                    loss = circle_loss(pos_sim, neg_sim)\n",
    "                    loss.backward()\n",
    "                    optimizers[k].step()\n",
    "\n",
    "                    batch_losses[k].append(loss.item())\n",
    "                    losses[k].append(loss.item())\n",
    "\n",
    "            if self.verbose:\n",
    "                avg_losses = {k: sum(batch_losses[k]) / len(batch_losses[k]) for k in k_values}\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Average Losses: {avg_losses}\")\n",
    "\n",
    "        return models, losses\n",
    "\n",
    "    def generate_embeddings(self, model, k, embeddings_to_use, batch_size=64):\n",
    "        padding_size = self.embeddings_concatenated.shape[1] - embeddings_to_use.shape[1]\n",
    "\n",
    "        dataset = RegionEmbeddingsDataset(self.regions_gdf.index.values, k, embeddings_to_use, padding_size)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "        model.eval()\n",
    "        output_embeddings = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for embeddings_batch in dataloader:\n",
    "                embeddings_batch = embeddings_batch.float().to(self.device)\n",
    "                output = model(embeddings_batch)\n",
    "                output_embeddings.append(output.cpu())\n",
    "    \n",
    "        all_output_embeddings = torch.cat(output_embeddings, dim=0)\n",
    "        output_embeddings_df = pd.DataFrame(all_output_embeddings.numpy(), index=self.regions_gdf.index[:all_output_embeddings.size(0)],\n",
    "                                            columns=[f\"dim_{i}\" for i in range(all_output_embeddings.size(1))])\n",
    "        return output_embeddings_df\n",
    "\n",
    "\n",
    "    def evaluate_embeddings(self, embeddings):\n",
    "        r2_scores = {}\n",
    "        target_columns = ['afw', 'vrz', 'fys', 'soc', 'onv', 'won']\n",
    "\n",
    "        for column in target_columns:\n",
    "            y = self.regions_gdf[column]\n",
    "            mask = ~(np.isnan(y) | np.isnan(embeddings).any(axis=1))\n",
    "            X_valid = embeddings[mask]\n",
    "            y_valid = y[mask]\n",
    "\n",
    "            if len(y_valid) == 0:\n",
    "                if self.verbose:\n",
    "                    print(f\"Warning: No valid data for {column} after removing NaN values.\")\n",
    "                r2_scores[column] = np.nan\n",
    "                continue\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_valid, y_valid, test_size=0.3, random_state=42)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2_scores[column] = r2_score(y_test, y_pred)\n",
    "\n",
    "        return r2_scores\n",
    "\n",
    "    def run_inference(self, models, k_values, data_sources):\n",
    "        results = {}\n",
    "        embeddings = {}\n",
    "        selected_embeddings = self.select_embeddings(data_sources)\n",
    "        for k in k_values:\n",
    "            embeddings[k] = self.generate_embeddings(models[k], k, selected_embeddings)\n",
    "            results[k] = self.evaluate_embeddings(embeddings[k])\n",
    "        return results, embeddings\n",
    "\n",
    "    def run_experiment(self, k_values, weight_types, data_sources, num_epochs=2):\n",
    "        results = {}\n",
    "        all_embeddings = {}\n",
    "        total_experiments = len(weight_types) * len(data_sources)\n",
    "    \n",
    "        with tqdm(total=total_experiments, desc='Experiment Progress', unit='experiment') as progress_bar:\n",
    "            for weight_type in weight_types:\n",
    "                models, losses = self.train_multiple_models(k_values, weight_type, num_epochs=num_epochs)\n",
    "    \n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "                    future_to_data_source = {\n",
    "                        executor.submit(self.run_inference, models, k_values, ds): ds\n",
    "                        for ds in data_sources\n",
    "                    }\n",
    "    \n",
    "                    for future in concurrent.futures.as_completed(future_to_data_source):\n",
    "                        ds = future_to_data_source[future]\n",
    "                        try:\n",
    "                            inference_results, inference_embeddings = future.result()\n",
    "                            for k in k_values:\n",
    "                                key = (k, weight_type, ds)\n",
    "                                results[key] = inference_results[k]\n",
    "                                all_embeddings[key] = inference_embeddings[k]\n",
    "                            progress_bar.update(1)\n",
    "                            progress_bar.set_description(f\"Completed: weight_type={weight_type}, data_sources={ds}\")\n",
    "                        except Exception as exc:\n",
    "                            print(f\"Data source {ds} generated an exception: {exc}\")\n",
    "    \n",
    "                # Cleanup\n",
    "                for model in models.values():\n",
    "                    model.cpu()\n",
    "                del models\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "        self.save_results(results, all_embeddings)\n",
    "        return results, all_embeddings\n",
    "\n",
    "    def save_results(self, results, all_embeddings):\n",
    "        with open(os.path.join(self.experiment_dir, 'results.json'), 'w') as f:\n",
    "            json.dump({str(k): v for k, v in results.items()}, f)\n",
    "        for key, embeddings in all_embeddings.items():\n",
    "            # Change this line\n",
    "            filename = f\"embeddings_k{key[0]}_{key[1]}_{key[2]}.csv\"\n",
    "            embeddings.to_csv(os.path.join(self.experiment_dir, filename))\n",
    "\n",
    "    def plot_results(self, results):\n",
    "        self.plot_k_rings_performance(results)\n",
    "        self.plot_weight_type_performance(results)\n",
    "        self.plot_data_source_performance(results)\n",
    "\n",
    "    def plot_k_rings_performance(self, results):\n",
    "        plt.figure(figsize=(16, 10))\n",
    "\n",
    "        k_values = sorted(set(key[0] for key in results.keys()))\n",
    "        weight_types = sorted(set(key[1] for key in results.keys()))\n",
    "\n",
    "        for target in self.target_columns:\n",
    "            for weight_type in weight_types:\n",
    "                r2_scores = []\n",
    "                for k in k_values:\n",
    "                    best_score = max(results.get((k, weight_type, ds), {}).get(target, -np.inf)\n",
    "                                     for ds in set(key[2] for key in results.keys()))\n",
    "                    r2_scores.append(best_score)\n",
    "\n",
    "                plt.plot(k_values, r2_scores,\n",
    "                         label=f\"{self.target_names[target]} ({weight_type})\",\n",
    "                         color=self.colors[target],\n",
    "                         marker=self.markers[weight_type],\n",
    "                         linestyle='-',\n",
    "                         linewidth=2,\n",
    "                         markersize=8)\n",
    "\n",
    "        plt.xlabel('Number of k-rings', fontsize=12)\n",
    "        plt.ylabel('Best R² Score', fontsize=12)\n",
    "        plt.title(f'Best Performance vs Number of k-rings (Resolution {self.resolution})', fontsize=14)\n",
    "\n",
    "        # Create a custom legend for target variables\n",
    "        legend_elements = []\n",
    "        for target in self.target_columns:\n",
    "            legend_elements.append(plt.Line2D([0], [0], color=self.colors[target], lw=4, label=self.target_names[target]))\n",
    "        plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10)\n",
    "\n",
    "        # Add a secondary legend for weight types\n",
    "        weight_legend = plt.legend([plt.Line2D([0], [0], marker=self.markers[wt], color='grey', linestyle='None', markersize=8)\n",
    "                                    for wt in weight_types],\n",
    "                                   weight_types,\n",
    "                                   loc='lower right',\n",
    "                                   title='Weight Types',\n",
    "                                   fontsize=8)\n",
    "        plt.gca().add_artist(weight_legend)\n",
    "\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.experiment_dir, 'k_rings_performance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_weight_type_performance(self, results):\n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=(16, 10), facecolor='white')\n",
    "        plt.gca().set_facecolor('white')\n",
    "    \n",
    "        weight_types = sorted(set(key[1] for key in results.keys()))\n",
    "        x = np.arange(len(weight_types))\n",
    "        width = 0.1\n",
    "    \n",
    "        for i, target in enumerate(self.target_columns):\n",
    "            best_r2 = [max(results[k, wt, ds][target]\n",
    "                           for k in set(key[0] for key in results.keys())\n",
    "                           for ds in set(key[2] for key in results.keys())\n",
    "                           if (k, wt, ds) in results)\n",
    "                       for wt in weight_types]\n",
    "    \n",
    "            plt.bar(x + i*width, best_r2, width, label=self.target_names[target],\n",
    "                    color=self.colors[target], edgecolor='none')\n",
    "    \n",
    "        plt.xlabel('Weighted Average Type', fontsize=12)\n",
    "        plt.ylabel('Best R² Score', fontsize=12)\n",
    "        plt.title(f'Best Performance vs Weighted Average Type (Resolution {self.resolution})', fontsize=14)\n",
    "        plt.xticks(x + width * (len(self.target_columns) - 1) / 2, weight_types, rotation=45, ha='right')\n",
    "        plt.legend(loc='upper right', fontsize=10)\n",
    "        plt.grid(axis='y', linestyle=':', color='gray', alpha=0.3)\n",
    "        plt.ylim(0, 1)  # Set y-axis limits from 0 to 1 for R² scores\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.experiment_dir, 'weight_type_performance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_data_source_performance(self, results):\n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=(16, 10), facecolor='white')\n",
    "        plt.gca().set_facecolor('white')\n",
    "    \n",
    "        data_sources = sorted(set(key[2] for key in results.keys()))\n",
    "        x = np.arange(len(data_sources))\n",
    "        width = 0.1\n",
    "    \n",
    "        for i, target in enumerate(self.target_columns):\n",
    "            best_r2 = [max(results[k, wt, ds][target]\n",
    "                           for k in set(key[0] for key in results.keys())\n",
    "                           for wt in set(key[1] for key in results.keys())\n",
    "                           if (k, wt, ds) in results)\n",
    "                       for ds in data_sources]\n",
    "    \n",
    "            plt.bar(x + i*width, best_r2, width, label=self.target_names[target],\n",
    "                    color=self.colors[target], edgecolor='none')\n",
    "    \n",
    "        plt.xlabel('Data Source', fontsize=12)\n",
    "        plt.ylabel('Best R² Score', fontsize=12)\n",
    "        plt.title(f'Best Performance vs Data Source (Resolution {self.resolution})', fontsize=14)\n",
    "        plt.xticks(x + width * (len(self.target_columns) - 1) / 2, data_sources, rotation=45, ha='right')\n",
    "        plt.legend(loc='best', fontsize=10)\n",
    "        plt.grid(axis='y', linestyle=':', color='gray', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.experiment_dir, 'data_source_performance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def save_experiment_info(self, k_values, weight_types, data_source_combinations):\n",
    "        info = {\n",
    "            'Date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'Resolution': self.resolution,\n",
    "            'Use Euclidean': self.use_euclidean,\n",
    "            'Number of k-ring values': len(k_values),\n",
    "            'Number of weight types': len(weight_types),\n",
    "            'Number of data source combinations': len(data_source_combinations),\n",
    "            'k-ring values': k_values,\n",
    "            'Weight types': weight_types,\n",
    "            'Data source combinations': [list(combo) if isinstance(combo, tuple) else combo for combo in data_source_combinations],\n",
    "            'Use finetuned aerial': self.use_finetuned_aerial,\n",
    "            'Use finetuned streetview': self.use_finetuned_streetview,\n",
    "            'Image PCA dimensions': self.image_pca_dim,\n",
    "            'POI embedding': self.poi_embedding,\n",
    "            'Threshold': 'bottom 2nd percentile if Euclidean, top 98th percentile if accessibility'\n",
    "        }\n",
    "\n",
    "        aggregation_nn_description = \"\"\"\n",
    "        Aggregation Neural Network Description:\n",
    "        - The aggregation neural network (RingAggregationNN) processes concatenated embeddings of various data sources.\n",
    "        - It consists of two sub-networks: WithinRingNN and BetweenRingNN.\n",
    "        - WithinRingNN architecture:\n",
    "          - BatchNorm -> Linear -> GELU -> Linear\n",
    "        - BetweenRingNN architecture:\n",
    "          - Linear -> GELU -> Linear\n",
    "        - WithinRingNN transforms and aggregates embeddings within a ring of neighbors.\n",
    "        - BetweenRingNN processes the pooled embeddings of different rings to generate the final output.\n",
    "        - The network utilizes batch normalization only in the WithinRingNN to normalize the input embeddings.\n",
    "        - GELU (Gaussian Error Linear Unit) activation is used instead of ReLU for improved performance and regularization.\n",
    "        \"\"\"\n",
    "    \n",
    "        with open(os.path.join(self.experiment_dir, 'experiment_info.txt'), 'w') as f:\n",
    "            for key, value in info.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(\"\\n\")  # Add a blank line for better readability\n",
    "            f.write(aggregation_nn_description)\n",
    "\n",
    "    def save_top_bottom_embeddings(self, results, all_embeddings, top_n=3):\n",
    "        print(f\"\\nSaving top and bottom embeddings...\")\n",
    "        avg_scores = {key: np.mean(list(scores.values())) for key, scores in results.items()}\n",
    "        sorted_configs = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        total_embeddings = len(sorted_configs)\n",
    "        save_n = min(top_n, total_embeddings // 2)\n",
    "        top_configs = sorted_configs[:save_n]\n",
    "        bottom_configs = sorted_configs[-save_n:]\n",
    "        best_worst_dir = os.path.join(self.experiment_dir, 'best_worst_embeddings')\n",
    "        os.makedirs(best_worst_dir, exist_ok=True)\n",
    "        for i, (config, score) in enumerate(top_configs, 1):\n",
    "            filename = os.path.join(best_worst_dir, f'top_{i}_embedding_k{config[0]}_{config[1]}_{\"_\".join(config[2])}.csv')\n",
    "            all_embeddings[config].to_csv(filename)\n",
    "            print(f\"Saved top {i} embedding to {filename} (Avg R² = {score:.4f})\")\n",
    "        for i, (config, score) in enumerate(bottom_configs, 1):\n",
    "            filename = os.path.join(best_worst_dir, f'bottom_{i}_embedding_k{config[0]}_{config[1]}_{\"_\".join(config[2])}.csv')\n",
    "            all_embeddings[config].to_csv(filename)\n",
    "            print(f\"Saved bottom {i} embedding to {filename} (Avg R² = {score:.4f})\")\n",
    "        print(f\"Finished saving {save_n} top and {save_n} bottom embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867c6645247b330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:25:24.107550Z",
     "start_time": "2024-07-25T11:48:39.149957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    resolution = 9\n",
    "    poi_embedding = 'hex2vec'\n",
    "    use_finetuned_aerial = False\n",
    "    use_finetuned_streetview = False\n",
    "    image_pca_dim = 100\n",
    "    use_euclidean = True  # True Euclidean distance - False location-based accessibility\n",
    "\n",
    "    # Experiment parameters\n",
    "    if resolution == 9:\n",
    "        k_values = [1, 3, 5]\n",
    "        all_data_sources = ['POI', 'roadnetwork', 'GTFS', 'aerial', 'streetview']\n",
    "    elif resolution == 10:\n",
    "        k_values = [1, 5, 10, 15]\n",
    "        all_data_sources = ['POI', 'roadnetwork', 'GTFS', 'aerial']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resolution: {resolution}\")\n",
    "    \n",
    "    experiment = LearntAggregationExperiment(\n",
    "        resolution=resolution,\n",
    "        use_euclidean=use_euclidean,  # Add this parameter\n",
    "        poi_embedding=poi_embedding,\n",
    "        use_finetuned_aerial=use_finetuned_aerial,\n",
    "        use_finetuned_streetview=use_finetuned_streetview,\n",
    "        image_pca_dim=image_pca_dim,\n",
    "        k_values=k_values \n",
    "    )\n",
    "\n",
    "    weight_types = ['exponential_e', 'logarithm', 'linear', 'flat']\n",
    "\n",
    "    # Define data sources\n",
    "    data_sources = ['all'] + all_data_sources\n",
    "\n",
    "    # Save experiment info\n",
    "    experiment.save_experiment_info(k_values, weight_types, data_sources)\n",
    "\n",
    "    print(f\"Starting experiments for resolution {resolution}...\")\n",
    "\n",
    "    # Run the experiments\n",
    "    results, all_embeddings = experiment.run_experiment(k_values, weight_types, data_sources, num_epochs=1)\n",
    "\n",
    "    print(\"All experiments completed. Processing results...\")\n",
    "\n",
    "    # Plot results\n",
    "    experiment.plot_results(results)\n",
    "\n",
    "    # Print detailed results\n",
    "    with open(os.path.join(experiment.experiment_dir, 'detailed_results.txt'), 'w') as f:\n",
    "        for key, scores in results.items():\n",
    "            f.write(f\"k={key[0]}, weight_type={key[1]}, data_sources={key[2]}\\n\")\n",
    "            for column, score in scores.items():\n",
    "                f.write(f\"  {experiment.target_names[column]}: R² = {score:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Get best performing configuration\n",
    "    best_key = max(results, key=lambda k: np.mean(list(results[k].values())))\n",
    "    best_score = np.mean(list(results[best_key].values()))\n",
    "\n",
    "    with open(os.path.join(experiment.experiment_dir, 'best_configuration.txt'), 'w') as f:\n",
    "        f.write(f\"Best configuration: k={best_key[0]}, weight_type={best_key[1]}, data_sources={best_key[2]}\\n\")\n",
    "        f.write(f\"Average R² score: {best_score:.4f}\\n\")\n",
    "\n",
    "    # Save best embeddings\n",
    "    best_embeddings = all_embeddings[best_key]\n",
    "    best_embeddings.to_csv(os.path.join(experiment.experiment_dir, 'best_embeddings.csv'))\n",
    "\n",
    "    # Save top and bottom embeddings\n",
    "    experiment.save_top_bottom_embeddings(results, all_embeddings, top_n=3)\n",
    "\n",
    "    # Print dimensionality information\n",
    "    with open(os.path.join(experiment.experiment_dir, 'dimensionality_info.txt'), 'w') as f:\n",
    "        f.write(f\"Final embedding dimensionality: {best_embeddings.shape[1]}\\n\")\n",
    "        f.write(f\"Original dimensionalities:\\n\")\n",
    "        for key, embeddings in all_embeddings.items():\n",
    "            f.write(f\"  k={key[0]}, weight_type={key[1]}, data_sources={key[2]}: {embeddings.shape[1]}\\n\")\n",
    "\n",
    "    print(f\"Experiment results saved in: {experiment.experiment_dir}\")\n",
    "    print(\"Experiment completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e05ddefa08bd0",
   "metadata": {},
   "source": [
    "Additional functionality for updating existing experiments with new plotting capabilities and recalculating R-squared scores using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137b8aba8f740c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T10:55:33.609530Z",
     "start_time": "2024-07-25T10:55:33.604226Z"
    }
   },
   "outputs": [],
   "source": [
    "# from Plotting import pca_plot\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# # plot top 1 embedding\n",
    "# experiment_dir = r\"D:\\tu delft\\Afstuderen\\Phase 5 learning strategy comparison\\experiments\\20240701_run12_res9_learnt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692ebccdd89d931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T10:55:33.614017Z",
     "start_time": "2024-07-25T10:55:33.610532Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# import os\n",
    "# import json\n",
    "# \n",
    "# # Load existing results\n",
    "# experiment_dir = r\"D:\\tu delft\\Afstuderen\\Phase 5 learning strategy comparison\\experiments\\20240702_run05_res9_learnt\"\n",
    "# with open(os.path.join(experiment_dir, 'results.json'), 'r') as f:\n",
    "#     results = json.load(f)\n",
    "# \n",
    "# # Convert string keys back to tuples\n",
    "# results = {eval(k): v for k, v in results.items()}\n",
    "# \n",
    "# # Create LearntAggregationExperiment instance\n",
    "# experiment = LearntAggregationExperiment(resolution=9)  # Adjust resolution as needed\n",
    "# \n",
    "# # Override the experiment directory with the existing one\n",
    "# experiment.experiment_dir = experiment_dir\n",
    "# \n",
    "# # Update plots\n",
    "# experiment.plot_results(results)\n",
    "# \n",
    "# print(f\"Updated plots saved in: {experiment_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750599496e75bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T10:55:33.618351Z",
     "start_time": "2024-07-25T10:55:33.615018Z"
    }
   },
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\")\n",
    "# \n",
    "# # Load existing results\n",
    "# experiment_dir = r\"D:\\tu delft\\Afstuderen\\Phase 5 learning strategy comparison\\experiments\\20240701_run12_res9_learnt\"\n",
    "# with open(os.path.join(experiment_dir, 'results.json'), 'r') as f:\n",
    "#     old_results = json.load(f)\n",
    "# \n",
    "# # Convert string keys back to tuples\n",
    "# old_results = {eval(k): v for k, v in old_results.items()}\n",
    "# \n",
    "# # Create LearntAggregationExperiment instance\n",
    "# experiment = LearntAggregationExperiment(resolution=9)  # Adjust resolution as needed\n",
    "# \n",
    "# # Override the experiment directory with the existing one\n",
    "# experiment.experiment_dir = experiment_dir\n",
    "# \n",
    "# # Recalculate R-squared scores with PCA\n",
    "# new_results = {}\n",
    "# \n",
    "# # Determine the number of components to use (minimum dimensionality of all data sources)\n",
    "# min_dim = min(emb.shape[1] for emb in experiment.embeddings.values())\n",
    "# \n",
    "# for params, _ in tqdm(old_results.items(), desc=\"Recalculating R-squared scores\"):\n",
    "#     k, weight_type, data_sources = params\n",
    "# \n",
    "#     # Load the corresponding embedding file\n",
    "#     filename = f\"embeddings_k{k}_{weight_type}_{'-'.join(data_sources)}.csv\"\n",
    "#     embedding_file = os.path.join(experiment_dir, filename)\n",
    "#     embeddings = pd.read_csv(embedding_file, index_col=0)\n",
    "# \n",
    "#     # Apply PCA\n",
    "#     pca = PCA(n_components=min_dim)\n",
    "#     X_pca = pca.fit_transform(embeddings)\n",
    "# \n",
    "#     r2_scores = {}\n",
    "#     for column in experiment.target_columns:\n",
    "#         y = experiment.regions_gdf[column]\n",
    "# \n",
    "#         mask = ~(np.isnan(y) | np.isnan(X_pca).any(axis=1))\n",
    "#         X_valid = X_pca[mask]\n",
    "#         y_valid = y[mask]\n",
    "# \n",
    "#         if len(y_valid) == 0:\n",
    "#             print(f\"Warning: No valid data for {column} after removing NaN values.\")\n",
    "#             r2_scores[column] = np.nan\n",
    "#             continue\n",
    "# \n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X_valid, y_valid, test_size=0.3, random_state=42)\n",
    "#         model = LinearRegression()\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         r2_scores[column] = r2_score(y_test, y_pred)\n",
    "# \n",
    "#     new_results[params] = r2_scores\n",
    "# \n",
    "# # Save updated results\n",
    "# with open(os.path.join(experiment_dir, 'results_with_pca.json'), 'w') as f:\n",
    "#     json.dump({str(k): v for k, v in new_results.items()}, f)\n",
    "# \n",
    "# # Plot updated results\n",
    "# experiment.plot_results(new_results)\n",
    "# \n",
    "# print(f\"Updated results and plots saved in: {experiment_dir}\")\n",
    "# \n",
    "# # Print best configuration\n",
    "# best_key = max(new_results, key=lambda k: np.mean(list(new_results[k].values())))\n",
    "# best_score = np.mean(list(new_results[best_key].values()))\n",
    "# \n",
    "# print(f\"\\nBest configuration: k={best_key[0]}, weight_type={best_key[1]}, data_sources={best_key[2]}\")\n",
    "# print(f\"Average R² score: {best_score:.4f}\")\n",
    "# \n",
    "# # Print detailed results for best configuration\n",
    "# print(\"\\nDetailed R² scores for best configuration:\")\n",
    "# for column, score in new_results[best_key].items():\n",
    "#     print(f\"  {experiment.target_names[column]}: R² = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f11396d7ee9ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T10:55:33.623919Z",
     "start_time": "2024-07-25T10:55:33.621853Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
