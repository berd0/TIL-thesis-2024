{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:54:11.152998Z",
     "start_time": "2024-07-25T08:54:04.874446Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "import torch as torch\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbe1dc30ccb69c",
   "metadata": {},
   "source": [
    "This code is a sped up version, made by claude sonnet 3.5, which uses poi, roadnetwork and gtfs embeddings to create a new embedding for each region. The new embedding is created by taking the exponential weighted average of the region's embedding and its neighbors' embeddings. The code is optimized to run on GPU and uses the H3 hexagonal grid to find neighbors of each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b9c12cafbfd09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:54:11.168051Z",
     "start_time": "2024-07-25T08:54:11.154004Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingProcessor:\n",
    "    def __init__(self, resolution, poi_embedding='geovex', use_finetuned_aerial=False, use_finetuned_streetview=False, image_pca_dim=None):\n",
    "        self.resolution = resolution\n",
    "        self.embeddings = {}\n",
    "        self.regions_buffered_gdf = None\n",
    "        self.regions_gdf = None\n",
    "        self.region_id_to_index = None\n",
    "        self.neighborhood = None\n",
    "        self.all_neighbors = None\n",
    "        self.poi_embedding = poi_embedding\n",
    "        self.use_finetuned_aerial = use_finetuned_aerial\n",
    "        self.use_finetuned_streetview = use_finetuned_streetview\n",
    "        self.image_pca_dim = image_pca_dim\n",
    "\n",
    "    def load_data(self):\n",
    "        print(f\"Loading data for resolution {self.resolution}...\")\n",
    "\n",
    "        # Load POI embeddings\n",
    "        poi_file = f'embeddings_POI_{self.poi_embedding}_{self.resolution}.csv'\n",
    "        self.embeddings['POI'] = pd.read_csv(poi_file, index_col='region_id')\n",
    "\n",
    "        # Common embeddings for both resolutions\n",
    "        common_embeddings = ['roadnetwork', 'GTFS']\n",
    "        for emb in common_embeddings:\n",
    "            self.embeddings[emb] = pd.read_csv(f'embeddings_{emb}_{self.resolution}.csv', index_col='region_id')\n",
    "\n",
    "        # Load and reduce aerial embeddings\n",
    "        aerial_file = f'embeddings_aerial_{self.resolution}'\n",
    "        if self.use_finetuned_aerial:\n",
    "            aerial_file += '_finetune'\n",
    "        aerial_file += '.csv'\n",
    "        self.embeddings['aerial'] = self._load_and_reduce_image_embedding(aerial_file)\n",
    "\n",
    "        # Resolution-specific embeddings (using ConvNext & only mean pooling of panoid per region_id)\n",
    "        if self.resolution == 9:\n",
    "            streetview_suffix = '_finetune' if self.use_finetuned_streetview else ''\n",
    "            #self.embeddings['streetview_max'] = self._load_and_reduce_image_embedding(f'embeddings_streetview_max_{self.resolution}{streetview_suffix}.csv')\n",
    "            self.embeddings['streetview_mean'] = self._load_and_reduce_image_embedding(f'embeddings_streetview_mean_{self.resolution}{streetview_suffix}.csv')\n",
    "\n",
    "        self.regions_buffered_gdf = gpd.read_file(f'selected_regions_buffered_{self.resolution}.geojson').set_index('region_id')\n",
    "        self.regions_gdf = gpd.read_file(f'selected_regions_{self.resolution}.geojson').set_index('region_id')\n",
    "\n",
    "        print(f\"Original shapes: regions_gdf: {self.regions_gdf.shape}, regions_buffered_gdf: {self.regions_buffered_gdf.shape}\")\n",
    "\n",
    "        # Handle duplicate indices\n",
    "        self.regions_buffered_gdf = self.regions_buffered_gdf[~self.regions_buffered_gdf.index.duplicated(keep='first')]\n",
    "        self.regions_gdf = self.regions_gdf[~self.regions_gdf.index.duplicated(keep='first')]\n",
    "\n",
    "        # Ensure all embeddings have representations for every region in the buffered area\n",
    "        for key in self.embeddings:\n",
    "            # Remove duplicate indices in embeddings\n",
    "            self.embeddings[key] = self.embeddings[key][~self.embeddings[key].index.duplicated(keep='first')]\n",
    "            # Reindex with the unique indices from regions_buffered_gdf\n",
    "            self.embeddings[key] = self.embeddings[key].reindex(self.regions_buffered_gdf.index, fill_value=0)\n",
    "\n",
    "        print(\"Data loaded and aligned successfully.\")\n",
    "        print(f\"Final shapes: regions_gdf: {self.regions_gdf.shape}, regions_buffered_gdf: {self.regions_buffered_gdf.shape}\")\n",
    "        for key, embedding in self.embeddings.items():\n",
    "            print(f\"Embedding {key} shape: {embedding.shape}\")\n",
    "\n",
    "    def _load_and_reduce_image_embedding(self, file_path):\n",
    "        # Read the CSV file, using the first column as the index\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "    \n",
    "        # Rename the index to 'region_id'\n",
    "        df.index.name = 'region_id'\n",
    "    \n",
    "        if self.image_pca_dim is not None:\n",
    "            pca = PCA(n_components=self.image_pca_dim)\n",
    "            reduced_data = pca.fit_transform(df)\n",
    "            return pd.DataFrame(reduced_data, index=df.index)\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self):\n",
    "        print(\"Preparing data...\")\n",
    "        self.region_id_to_index = {region_id: idx for idx, region_id in enumerate(self.regions_gdf.index)}\n",
    "        self.neighborhood = H3Neighbourhood(self.regions_buffered_gdf)\n",
    "        print(\"Data prepared.\")\n",
    "\n",
    "    def get_all_neighbors(self, k):\n",
    "        print(f\"Calculating neighbors for k={k}...\")\n",
    "        self.all_neighbors = {}\n",
    "        for region_id in self.regions_gdf.index:\n",
    "            self.all_neighbors[region_id] = [list(self.neighborhood.get_neighbours_at_distance(str(region_id), i)) for i in range(1, k+1)]\n",
    "        print(\"Neighbors calculated.\")\n",
    "\n",
    "    def get_embeddings_for_region(self, region_id, k, embeddings_tensor):\n",
    "        region_embeddings = []\n",
    "        region_index = self.region_id_to_index[region_id]\n",
    "\n",
    "        region_embeddings.append(embeddings_tensor[region_index])\n",
    "\n",
    "        for i in range(1, k + 1):\n",
    "            neighbor_ids = self.all_neighbors[region_id][i-1]\n",
    "            neighbor_indices = [self.regions_buffered_gdf.index.get_loc(neighbor_id) for neighbor_id in neighbor_ids if neighbor_id in self.regions_buffered_gdf.index]\n",
    "            if neighbor_indices:\n",
    "                embeddings = embeddings_tensor[neighbor_indices].mean(dim=0)\n",
    "            else:\n",
    "                embeddings = torch.zeros(embeddings_tensor.shape[1])\n",
    "            region_embeddings.append(embeddings)\n",
    "\n",
    "        return torch.stack(region_embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_weighted_average(embeddings, weight_type):\n",
    "        num_embeddings = len(embeddings)\n",
    "    \n",
    "        if weight_type == 'exponential_e':\n",
    "            weights = torch.exp(torch.arange(num_embeddings, dtype=torch.float32) * -1)\n",
    "        elif weight_type == 'logarithm':\n",
    "            weights = 1 / torch.log2(torch.arange(num_embeddings, dtype=torch.float32) + 2)\n",
    "        elif weight_type == 'linear':\n",
    "            weights = torch.linspace(1, 0, num_embeddings)\n",
    "        elif weight_type == 'flat':\n",
    "            weights = torch.ones(num_embeddings, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported weight_type: {weight_type}\")\n",
    "    \n",
    "        # Ensure the first weight (corresponding to ring 0) is the highest by reversing the weights\n",
    "        weights = weights.flip(0)\n",
    "    \n",
    "        # Calculate the weighted sum of embeddings\n",
    "        weighted_sum = torch.sum(embeddings * weights.unsqueeze(1), dim=0)\n",
    "    \n",
    "        # Return the normalized weighted sum\n",
    "        return weighted_sum / weights.sum()\n",
    "\n",
    "    def process_embeddings(self, k=15, weight_type='exponential_e', data_source='all'):\n",
    "        if self.all_neighbors is None or len(next(iter(self.all_neighbors.values()))) != k:\n",
    "            self.get_all_neighbors(k)\n",
    "\n",
    "        if data_source == 'all':\n",
    "            embeddings_concatenated = pd.concat(list(self.embeddings.values()), axis=1)\n",
    "        elif data_source in self.embeddings:\n",
    "            embeddings_concatenated = self.embeddings[data_source]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data_source: {data_source}\")\n",
    "\n",
    "        embeddings_tensor = torch.tensor(embeddings_concatenated.values, dtype=torch.float32)\n",
    "\n",
    "        all_region_embeddings = []\n",
    "\n",
    "        for region_id in self.regions_gdf.index:\n",
    "            region_embeddings = self.get_embeddings_for_region(region_id, k, embeddings_tensor)\n",
    "            weighted_embedding = self.calculate_weighted_average(region_embeddings, weight_type)\n",
    "            all_region_embeddings.append(weighted_embedding.numpy())\n",
    "\n",
    "        output_embeddings_df = pd.DataFrame(all_region_embeddings, index=self.regions_gdf.index)\n",
    "        return output_embeddings_df\n",
    "\n",
    "        for region_id in tqdm(self.regions_gdf.index, desc=\"Processing regions\"):\n",
    "            region_embeddings = self.get_embeddings_for_region(region_id, k, embeddings_tensor)\n",
    "            weighted_embedding = self.calculate_weighted_average(region_embeddings, weight_type)\n",
    "            all_region_embeddings.append(weighted_embedding.numpy())\n",
    "\n",
    "        output_embeddings_df = pd.DataFrame(all_region_embeddings, index=self.regions_gdf.index)\n",
    "        print(\"Embedding processing complete.\")\n",
    "        return output_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867c6645247b330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:54:11.195266Z",
     "start_time": "2024-07-25T08:54:11.169058Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingExperiment:\n",
    "    def __init__(self, resolution, poi_embedding='geovex', use_finetuned_aerial=False, use_finetuned_streetview=False, image_pca_dim=None):\n",
    "        self.resolution = resolution\n",
    "        self.processor = EmbeddingProcessor(resolution,\n",
    "                                            poi_embedding=poi_embedding,\n",
    "                                            use_finetuned_aerial=use_finetuned_aerial,\n",
    "                                            use_finetuned_streetview=use_finetuned_streetview,\n",
    "                                            image_pca_dim=image_pca_dim)\n",
    "        self.processor.load_data()\n",
    "        self.processor.prepare_data()\n",
    "        self.experiment_dir = self.create_experiment_directory()\n",
    "\n",
    "\n",
    "        self.target_columns = ['afw', 'vrz', 'fys', 'soc', 'onv', 'won']\n",
    "        self.target_names = {\n",
    "            'afw': 'Liveability',\n",
    "            'vrz': 'Amenities',\n",
    "            'fys': 'Physical Environment',\n",
    "            'soc': 'Social Cohesion',\n",
    "            'onv': 'Safety',\n",
    "            'won': 'Housing Stock'\n",
    "        }\n",
    "        self.colors = {\n",
    "            'afw': '#808080',  # Dark Grey for Liveability\n",
    "            'vrz': '#FF4500',  # Orange Red for Amenities\n",
    "            'fys': '#32CD32',  # Lime Green for Physical Environment\n",
    "            'soc': '#8A2BE2',  # Blue Violet for Social Cohesion\n",
    "            'onv': '#1E90FF',  # Dodger Blue for Safety\n",
    "            'won': '#FFA500'   # Orange for Housing Stock\n",
    "        }\n",
    "        self.markers = {\n",
    "            'exponential_e': 'o',\n",
    "            'linear': 's',\n",
    "            'flat': '^',\n",
    "            'logarithm': 'D'\n",
    "        }\n",
    "        \n",
    "    def create_experiment_directory(self):\n",
    "        date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "        base_dir = \"experiments\"\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "        run_number = 1\n",
    "        while True:\n",
    "            dir_name = f\"{date_str}_run{run_number:02d}_res{self.resolution}\"\n",
    "            full_path = os.path.join(base_dir, dir_name)\n",
    "            if not os.path.exists(full_path):\n",
    "                os.makedirs(full_path)\n",
    "                return full_path\n",
    "            run_number += 1\n",
    "\n",
    "    def run_experiment(self, k_values, weight_types, data_sources):\n",
    "        params_list = [(k, wt, ds) for k in k_values for wt in weight_types for ds in data_sources]\n",
    "        total_experiments = len(params_list)\n",
    "\n",
    "        print(f\"Starting experiments with {total_experiments} parameter combinations...\")\n",
    "\n",
    "        all_embeddings = {}\n",
    "        all_results = {}\n",
    "\n",
    "        # Suppress warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        # Use a single progress bar for all experiments\n",
    "        with tqdm(total=total_experiments, desc=\"Experiments Progress\", ncols=100) as pbar:\n",
    "            for params in params_list:\n",
    "                k, weight_type, data_source = params\n",
    "\n",
    "                try:\n",
    "                    result = self.run_single_experiment(params)\n",
    "                    all_embeddings[params] = result[1]\n",
    "                    all_results[params] = result[2]\n",
    "                except Exception as exc:\n",
    "                    print(f\"\\nExperiment {params} generated an exception: {exc}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Completed: k={k}, weight={weight_type}, source={data_source}\")\n",
    "\n",
    "        # Re-enable warnings\n",
    "        warnings.resetwarnings()\n",
    "\n",
    "        self.save_results(all_results, all_embeddings)\n",
    "        return all_results, all_embeddings\n",
    "\n",
    "    def run_single_experiment(self, params):\n",
    "        k, weight_type, data_source = params\n",
    "        embeddings = self.processor.process_embeddings(k=k, weight_type=weight_type, data_source=data_source)\n",
    "        r2_scores = self.evaluate_embeddings(embeddings)\n",
    "        return (k, weight_type, data_source), embeddings, r2_scores\n",
    "\n",
    "    def evaluate_embeddings(self, embeddings):\n",
    "        r2_scores = {}\n",
    "\n",
    "        # Determine the number of components to use (minimum dimensionality of all data sources)\n",
    "        min_dim = min(emb.shape[1] for emb in self.processor.embeddings.values())\n",
    "\n",
    "        # Apply PCA & print the total explained variance ratio\n",
    "        pca = PCA(n_components=min_dim)\n",
    "        X_pca = pca.fit_transform(embeddings)\n",
    "    \n",
    "        for column in self.target_columns:\n",
    "            y = self.processor.regions_gdf[column]\n",
    "\n",
    "            mask = ~(np.isnan(y) | np.isnan(X_pca).any(axis=1))\n",
    "            X_valid = X_pca[mask]\n",
    "            y_valid = y[mask]\n",
    "\n",
    "            if len(y_valid) == 0:\n",
    "                print(f\"Warning: No valid data for {column} after removing NaN values.\")\n",
    "                r2_scores[column] = np.nan\n",
    "                continue\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_valid, y_valid, test_size=0.3, random_state=42)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2_scores[column] = r2_score(y_test, y_pred)\n",
    "        return r2_scores\n",
    "\n",
    "    def save_results(self, results, all_embeddings):\n",
    "        with open(os.path.join(self.experiment_dir, 'results.json'), 'w') as f:\n",
    "            json.dump({str(k): v for k, v in results.items()}, f)\n",
    "        for key, embeddings in all_embeddings.items():\n",
    "            filename = f\"embeddings_k{key[0]}_{key[1]}_{key[2]}.csv\"\n",
    "            embeddings.to_csv(os.path.join(self.experiment_dir, filename))\n",
    "\n",
    "    def load_results(self):\n",
    "        with open(os.path.join(self.experiment_dir, 'results.json'), 'r') as f:\n",
    "            results = json.load(f)\n",
    "        return {eval(k): v for k, v in results.items()}\n",
    "\n",
    "    def plot_results(self, results):\n",
    "        self.plot_k_rings_performance(results)\n",
    "        self.plot_weight_type_performance(results)\n",
    "        self.plot_data_source_performance(results)\n",
    "\n",
    "    def plot_k_rings_performance(self, results):\n",
    "        plt.figure(figsize=(16, 10))\n",
    "\n",
    "        k_values = sorted(set(key[0] for key in results.keys()))\n",
    "        weight_types = sorted(set(key[1] for key in results.keys()))\n",
    "\n",
    "        for target in self.target_columns:\n",
    "            for weight_type in weight_types:\n",
    "                r2_scores = []\n",
    "                for k in k_values:\n",
    "                    best_score = max(results.get((k, weight_type, ds), {}).get(target, -np.inf)\n",
    "                                     for ds in set(key[2] for key in results.keys()))\n",
    "                    r2_scores.append(best_score)\n",
    "\n",
    "                plt.plot(k_values, r2_scores,\n",
    "                         label=f\"{self.target_names[target]} ({weight_type})\",\n",
    "                         color=self.colors[target],\n",
    "                         marker=self.markers[weight_type],\n",
    "                         linestyle='-',\n",
    "                         linewidth=2,\n",
    "                         markersize=8)\n",
    "\n",
    "        plt.xlabel('Number of k-rings', fontsize=12)\n",
    "        plt.ylabel('Best R² Score', fontsize=12)\n",
    "        plt.title(f'Best Performance vs Number of k-rings (Resolution {self.resolution})', fontsize=14)\n",
    "\n",
    "        # Create a custom legend for target variables\n",
    "        legend_elements = []\n",
    "        for target in self.target_columns:\n",
    "            legend_elements.append(plt.Line2D([0], [0], color=self.colors[target], lw=4, label=self.target_names[target]))\n",
    "        plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10)\n",
    "\n",
    "        # Add a secondary legend for weight types\n",
    "        weight_legend = plt.legend([plt.Line2D([0], [0], marker=self.markers[wt], color='grey', linestyle='None', markersize=8)\n",
    "                                    for wt in weight_types],\n",
    "                                   weight_types,\n",
    "                                   loc='lower right',\n",
    "                                   title='Weight Types',\n",
    "                                   fontsize=8)\n",
    "        plt.gca().add_artist(weight_legend)\n",
    "\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.experiment_dir, 'k_rings_performance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_weight_type_performance(self, results):\n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=(16, 10), facecolor='white')\n",
    "        plt.gca().set_facecolor('white')\n",
    "\n",
    "        weight_types = sorted(set(key[1] for key in results.keys()))\n",
    "        x = np.arange(len(weight_types))\n",
    "        width = 0.1\n",
    "\n",
    "        for i, target in enumerate(self.target_columns):\n",
    "            best_r2 = [max(results[k, wt, ds][target]\n",
    "                           for k in set(key[0] for key in results.keys())\n",
    "                           for ds in set(key[2] for key in results.keys())\n",
    "                           if (k, wt, ds) in results)\n",
    "                       for wt in weight_types]\n",
    "\n",
    "            plt.bar(x + i*width, best_r2, width, label=self.target_names[target],\n",
    "                    color=self.colors[target], edgecolor='none')\n",
    "\n",
    "        plt.xlabel('Weighted Average Type', fontsize=12)\n",
    "        plt.ylabel('Best R² Score', fontsize=12)\n",
    "        plt.title(f'Best Performance vs Weighted Average Type (Resolution {self.resolution})', fontsize=14)\n",
    "        plt.xticks(x + width * (len(self.target_columns) - 1) / 2, weight_types, rotation=45, ha='right')\n",
    "        plt.legend(loc='upper right', fontsize=10)\n",
    "        plt.grid(axis='y', linestyle=':', color='gray', alpha=0.3)\n",
    "        plt.ylim(0, 1)  # Set y-axis limits from 0 to 1 for R² scores\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.experiment_dir, 'weight_type_performance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_data_source_performance(self, results):\n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=(16, 10), facecolor='white')\n",
    "        plt.gca().set_facecolor('white')\n",
    "\n",
    "        data_sources = sorted(set(key[2] for key in results.keys()))\n",
    "        x = np.arange(len(data_sources))\n",
    "        width = 0.1\n",
    "\n",
    "        for i, target in enumerate(self.target_columns):\n",
    "            best_r2 = [max(results[k, wt, ds][target]\n",
    "                           for k in set(key[0] for key in results.keys())\n",
    "                           for wt in set(key[1] for key in results.keys())\n",
    "                           if (k, wt, ds) in results)\n",
    "                       for ds in data_sources]\n",
    "\n",
    "            plt.bar(x + i*width, best_r2, width, label=self.target_names[target],\n",
    "                    color=self.colors[target], edgecolor='none')\n",
    "\n",
    "        plt.xlabel('Data Source', fontsize=12)\n",
    "        plt.ylabel('Best R² Score', fontsize=12)\n",
    "        plt.title(f'Best Performance vs Data Source (Resolution {self.resolution})', fontsize=14)\n",
    "        plt.xticks(x + width * (len(self.target_columns) - 1) / 2, data_sources, rotation=45, ha='right')\n",
    "        plt.legend(loc='best', fontsize=10)\n",
    "        plt.grid(axis='y', linestyle=':', color='gray', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.experiment_dir, 'data_source_performance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def save_top_bottom_embeddings(self, results, all_embeddings, top_n=3):\n",
    "        print(f\"\\nSaving top and bottom embeddings...\")\n",
    "        avg_scores = {key: np.mean(list(scores.values())) for key, scores in results.items()}\n",
    "        sorted_configs = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        total_embeddings = len(sorted_configs)\n",
    "        save_n = min(top_n, total_embeddings // 2)\n",
    "        top_configs = sorted_configs[:save_n]\n",
    "        bottom_configs = sorted_configs[-save_n:]\n",
    "        best_worst_dir = os.path.join(self.experiment_dir, 'best_worst_embeddings')\n",
    "        os.makedirs(best_worst_dir, exist_ok=True)\n",
    "        for i, (config, score) in enumerate(top_configs, 1):\n",
    "            filename = os.path.join(best_worst_dir, f'top_{i}_embedding_k{config[0]}_{config[1]}_{config[2]}.csv')\n",
    "            all_embeddings[config].to_csv(filename)\n",
    "            print(f\"Saved top {i} embedding to {filename} (Avg R² = {score:.4f})\")\n",
    "        for i, (config, score) in enumerate(bottom_configs, 1):\n",
    "            filename = os.path.join(best_worst_dir, f'bottom_{i}_embedding_k{config[0]}_{config[1]}_{config[2]}.csv')\n",
    "            all_embeddings[config].to_csv(filename)\n",
    "            print(f\"Saved bottom {i} embedding to {filename} (Avg R² = {score:.4f})\")\n",
    "        print(f\"Finished saving {save_n} top and {save_n} bottom embeddings.\")\n",
    "\n",
    "    def save_experiment_info(self, k_values, weight_types, data_sources):\n",
    "        info = {\n",
    "            'Date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'Resolution': self.resolution,\n",
    "            'POI embedding type': self.processor.poi_embedding,\n",
    "            'Use finetuned aerial': self.processor.use_finetuned_aerial,\n",
    "            'Use finetuned streetview': self.processor.use_finetuned_streetview,\n",
    "            'Number of k-ring values': len(k_values),\n",
    "            'Number of weight types': len(weight_types),\n",
    "            'Number of data sources': len(data_sources),\n",
    "            'k-ring values': k_values,\n",
    "            'Weight types': weight_types,\n",
    "            'Data sources': data_sources\n",
    "        }\n",
    "        with open(os.path.join(self.experiment_dir, 'experiment_info.txt'), 'w') as f:\n",
    "            for key, value in info.items():\n",
    "                f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742d57d9a34ba1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T09:11:24.287211Z",
     "start_time": "2024-07-25T09:00:08.258869Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    resolution = 9  # Change this to 9 if you want to run for resolution 9\n",
    "    poi_embedding = 'hex2vec'  # Choose between 'geovex' and 'hex2vec'\n",
    "    use_finetuned_aerial = False  # Set to True to use finetuned aerial embeddings\n",
    "    use_finetuned_streetview = False  # Set to True to use finetuned streetview embeddings (only for resolution 9)\n",
    "    image_pca_dim = 100  # Set to None to use original dimensionality\n",
    "\n",
    "    experiment = EmbeddingExperiment(resolution,\n",
    "                                     poi_embedding=poi_embedding,\n",
    "                                     use_finetuned_aerial=use_finetuned_aerial,\n",
    "                                     use_finetuned_streetview=use_finetuned_streetview,\n",
    "                                     image_pca_dim=image_pca_dim)\n",
    "\n",
    "    # Experiment parameters\n",
    "    if resolution == 9:\n",
    "        k_values = [1, 2, 3, 4, 5]\n",
    "    elif resolution == 10:\n",
    "        k_values = [1, 3, 5, 7, 10, 15]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resolution: {resolution}\")\n",
    "\n",
    "    weight_types = ['exponential_e', 'logarithm', 'linear', 'flat']\n",
    "\n",
    "    # Define data sources based on resolution\n",
    "    if resolution == 9:\n",
    "        data_sources = ['all', 'POI', 'roadnetwork', 'GTFS', 'aerial', 'streetview_mean']\n",
    "    elif resolution == 10:\n",
    "        data_sources = ['all', 'POI', 'roadnetwork', 'GTFS', 'aerial']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resolution: {resolution}\")\n",
    "\n",
    "    # Save experiment info\n",
    "    experiment.save_experiment_info(k_values, weight_types, data_sources)\n",
    "\n",
    "    print(f\"Starting sequential experiments for resolution {resolution}...\")\n",
    "\n",
    "    # Run the sequential experiments\n",
    "    results, all_embeddings = experiment.run_experiment(k_values, weight_types, data_sources)\n",
    "\n",
    "    print(\"All experiments completed. Processing results...\")\n",
    "\n",
    "    # Plot results\n",
    "    experiment.plot_results(results)\n",
    "\n",
    "    # Print detailed results\n",
    "    with open(os.path.join(experiment.experiment_dir, 'detailed_results.txt'), 'w') as f:\n",
    "        for key, scores in results.items():\n",
    "            f.write(f\"k={key[0]}, weight_type={key[1]}, data_source={key[2]}\\n\")\n",
    "            for column, score in scores.items():\n",
    "                f.write(f\"  {experiment.target_names[column]}: R² = {score:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Get best performing configuration\n",
    "    best_key = max(results, key=lambda k: np.mean(list(results[k].values())))\n",
    "    best_score = np.mean(list(results[best_key].values()))\n",
    "\n",
    "    with open(os.path.join(experiment.experiment_dir, 'best_configuration.txt'), 'w') as f:\n",
    "        f.write(f\"Best configuration: k={best_key[0]}, weight_type={best_key[1]}, data_source={best_key[2]}\\n\")\n",
    "        f.write(f\"Average R² score: {best_score:.4f}\\n\")\n",
    "\n",
    "    # Save best embeddings\n",
    "    best_embeddings = all_embeddings[best_key]\n",
    "    best_embeddings.to_csv(os.path.join(experiment.experiment_dir, 'best_embeddings.csv'))\n",
    "\n",
    "    # Save top and bottom embeddings\n",
    "    experiment.save_top_bottom_embeddings(results, all_embeddings, top_n=3)\n",
    "\n",
    "    # Print dimensionality information\n",
    "    with open(os.path.join(experiment.experiment_dir, 'dimensionality_info.txt'), 'w') as f:\n",
    "        f.write(f\"Final embedding dimensionality: {best_embeddings.shape[1]}\\n\")\n",
    "        f.write(f\"Original dimensionalities:\\n\")\n",
    "        for key, embeddings in all_embeddings.items():\n",
    "            f.write(f\"  k={key[0]}, weight_type={key[1]}, data_source={key[2]}: {embeddings.shape[1]}\\n\")\n",
    "\n",
    "    print(f\"Experiment results saved in: {experiment.experiment_dir}\")\n",
    "    print(\"Experiment completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c24da85d86589",
   "metadata": {},
   "source": [
    "Recalculate R squared values with updated evaluation method (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee7195f0401ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:59:59.469607Z",
     "start_time": "2024-07-25T08:59:59.469108Z"
    }
   },
   "outputs": [],
   "source": [
    "# # disable warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# \n",
    "# # Load existing results\n",
    "# experiment_dir = r\"D:\\tu delft\\Afstuderen\\Phase 5 learning strategy comparison\\experiments\\20240701_run07\"\n",
    "# with open(os.path.join(experiment_dir, 'results.json'), 'r') as f:\n",
    "#     old_results = json.load(f)\n",
    "# \n",
    "# # Convert string keys back to tuples\n",
    "# old_results = {eval(k): v for k, v in old_results.items()}\n",
    "# \n",
    "# # Create EmbeddingExperiment instance\n",
    "# experiment = EmbeddingExperiment(resolution=10)     # DO NOT FORGET TO CHANGE RESOLUTION IF NEEDED\n",
    "# \n",
    "# # Override the experiment directory with the existing one\n",
    "# experiment.experiment_dir = experiment_dir\n",
    "# \n",
    "# # Recalculate R-squared scores with PCA\n",
    "# new_results = {}\n",
    "# \n",
    "# # Determine the number of components to use (minimum dimensionality of all data sources)\n",
    "# min_dim = min(emb.shape[1] for emb in experiment.processor.embeddings.values())\n",
    "# \n",
    "# for params, _ in tqdm(old_results.items(), desc=\"Recalculating R-squared scores\"):\n",
    "#     k, weight_type, data_source = params\n",
    "# \n",
    "#     # Load the corresponding embedding file\n",
    "#     embedding_file = os.path.join(experiment_dir, f\"embeddings_k{k}_{weight_type}_{data_source}.csv\")\n",
    "#     embeddings = pd.read_csv(embedding_file, index_col=0)\n",
    "# \n",
    "#     # Apply PCA\n",
    "#     pca = PCA(n_components=min_dim)\n",
    "#     X_pca = pca.fit_transform(embeddings)\n",
    "# \n",
    "#     r2_scores = {}\n",
    "#     for column in experiment.target_columns:\n",
    "#         y = experiment.processor.regions_gdf[column]\n",
    "# \n",
    "#         mask = ~(np.isnan(y) | np.isnan(X_pca).any(axis=1))\n",
    "#         X_valid = X_pca[mask]\n",
    "#         y_valid = y[mask]\n",
    "# \n",
    "#         if len(y_valid) == 0:\n",
    "#             print(f\"Warning: No valid data for {column} after removing NaN values.\")\n",
    "#             r2_scores[column] = np.nan\n",
    "#             continue\n",
    "# \n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X_valid, y_valid, test_size=0.3, random_state=42)\n",
    "#         model = LinearRegression()\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         r2_scores[column] = r2_score(y_test, y_pred)\n",
    "# \n",
    "#     new_results[params] = r2_scores\n",
    "# \n",
    "# # Save updated results\n",
    "# with open(os.path.join(experiment_dir, 'results_with_pca.json'), 'w') as f:\n",
    "#     json.dump({str(k): v for k, v in new_results.items()}, f)\n",
    "# \n",
    "# # Plot updated results\n",
    "# experiment.plot_results(new_results)\n",
    "# \n",
    "# print(f\"Updated results and plots saved in: {experiment_dir}\")\n",
    "# \n",
    "# # Print best configuration\n",
    "# best_key = max(new_results, key=lambda k: np.mean(list(new_results[k].values())))\n",
    "# best_score = np.mean(list(new_results[best_key].values()))\n",
    "# \n",
    "# print(f\"\\nBest configuration: k={best_key[0]}, weight_type={best_key[1]}, data_source={best_key[2]}\")\n",
    "# print(f\"Average R² score: {best_score:.4f}\")\n",
    "# \n",
    "# # Print detailed results for best configuration\n",
    "# print(\"\\nDetailed R² scores for best configuration:\")\n",
    "# for column, score in new_results[best_key].items():\n",
    "#     print(f\"  {experiment.target_names[column]}: R² = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3e62b7c6dde86",
   "metadata": {},
   "source": [
    "Recalculate plots using refactored code which takes best results for streamlined comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ea63d12547826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing results\n",
    "experiment_dir = r\"D:\\tu delft\\Afstuderen\\Phase 5 learning strategy comparison\\experiments\\20240701_run01_res9\"  \n",
    "with open(os.path.join(experiment_dir, 'results.json'), 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Convert string keys back to tuples\n",
    "results = {eval(k): v for k, v in results.items()}\n",
    "\n",
    "# Create EmbeddingExperiment instance\n",
    "experiment = EmbeddingExperiment(resolution=9)\n",
    "\n",
    "# Override the experiment directory with the existing one\n",
    "experiment.experiment_dir = experiment_dir\n",
    "\n",
    "# Plot results\n",
    "experiment.plot_results(results)\n",
    "\n",
    "print(f\"Updated plots saved in: {experiment_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5dd858832e2988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
